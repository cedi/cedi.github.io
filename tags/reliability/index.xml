<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>reliability on cedi.dev</title><link>https://cedi.dev/tags/reliability/</link><description>Recent content in reliability on cedi.dev</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} by Cedric Kienzler</copyright><lastBuildDate>Tue, 22 Aug 2023 20:00:00 +0000</lastBuildDate><atom:link href="https://cedi.dev/tags/reliability/index.xml" rel="self" type="application/rss+xml"/><item><title>Breaking Clouds</title><link>https://cedi.dev/post/breaking-clouds/</link><pubDate>Tue, 22 Aug 2023 20:00:00 +0000</pubDate><guid>https://cedi.dev/post/breaking-clouds/</guid><description>&lt;p>Cloud infrastructure is a necessity in our modern digital world. However, understanding and preparing for failures in cloud infrastructure is critical for reliability of our services. Failures can be viewed as learning opportunities and to improve our system design. It can inform proactive problem-solving, fostering effective incident response, and guiding future design challenges. &lt;a href="https://en.wikipedia.org/wiki/Chaos_engineering">Chaos Engineering&lt;/a> plays a vital role in testing for resilience of our system.&lt;/p>
&lt;h2 id="the-cloud-as-our-new-reality">The cloud as our new reality&lt;/h2>
&lt;p>Cloud Infrastructure has become an integral part of our modern digital landscape. It’s the foundation that supports a majority of the digital services used by millions of people every day. From social media platforms to online video conferencing enabling hybrid work. Even sectors like healthcare or education, cloud infrastructure plays a vital role these days.
Due to the cost-effectiveness and easy accessibility it has become easier and easier building more and more complex systems with ease and minimal knowledge required. Where once tens of infrastructure engineers maintained dozens of servers in a rack, nowadays any single developer can spin up vast infrastructure landscapes with intricate dependencies on public cloud-providers such as Azure or GCP.&lt;/p>
&lt;p>However this comes with the expectation of reliability and resilience. In this context, reliability means that cloud services should be available and function correctly when users need them.
Resilience means that these services can quickly recover from any disruption or failure.
Given our modern reality, a significant part of our everyday lives is now “in the cloud”. We rely on it through our entire live.
From communication both private and professional, the digital workplace, as well as entertainment, and even critical services such as healthcare set the expectations high when it comes to reliability and resilience.&lt;/p>
&lt;p>This is why understanding and preparing for potential failures in cloud infrastructure is so critical.&lt;/p>
&lt;h2 id="exploring-failures">Exploring failures&lt;/h2>
&lt;p>Failure modes, or commonly just called &amp;ldquo;failures&amp;rdquo;, describe the various ways in which a system, or service fails.
Failure modes are as diverse as the landscape of modern cloud infrastructure itself. They can range from Hardware to Software and even &lt;a href="https://youtu.be/Oev_PJm2yUQ?t=108">Human Failure&lt;/a>.
I think Hardware, Software failures are quite self explanatory: These involve the failure of hardware, like servers, routers, or storage devices, or bugs and glitches in the software that cause system malfunctions. But the effect is always the same: Users seeing errors, dropped connections, degraded system performance or intermittent problems of all sorts.
But to me, the most interesting failure modes are failures that involve the human factor. These are failures that happen when humans are involved in a process, like during incident response for example, and they make a mistake.
Human factors are responsible for an entire new spectrum of failure modes. From unplugging the wrong cable in the data center during a routine maintenance to configuring something that just does not work to executing the a destructive command in the wrong terminal window connected to the wrong server (Looking at you &lt;a href="https://about.gitlab.com/blog/2017/02/01/gitlab-dot-com-database-incident/#third-incident">GitLab&lt;/a>).&lt;/p>
&lt;p>By considering how individual components might fail we can develop a better design that is resilient to those failures.
We can use the &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a> to better understand the relationship between what the system actually is and how we think of it. We can remember that incidents inform us how the system actually behaves and how our assumptions about it where wrong. And gauging this delta enables us to take proactive measures to prevent such failures in the future, or at least make them less likely.
If you are a software developer you are probably familiar with the process of Pull Request reviews. They are a great tool and serve multiple functions that might not be obvious at first. The obvious benefit of Pull Request is that the code is screened to be bug free. But it also ensures that the mental model of the person who submits the pull request was correct and the correct assumptions were made when changing the code to not cause any unwanted side effects. And finally, it also serves as a vessel to update the mental model of the person who reviews the pull request by forcing that person to understand how the system behavior will change by this code change.&lt;/p>
&lt;p>Enough talking about Pull Requests and back to the topic of resiliency engineering. When we have an understanding of how a system might fail, we can take better informed decisions and have streamlined incident response processes in place help speeding up recovery. It enables engineers to more quickly diagnose and address the root cause of an incident with less friction.&lt;/p>
&lt;h2 id="failures-as-learning-opportunities">Failures as Learning Opportunities&lt;/h2>
&lt;p>While system failure is often viewed as a negative, it’s time to challenge this perspective. Instead we should consider these disruptions as an opportunity to improve our own mental model about how the system works (see &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a>) and inform our decisions going forward helping us to improve our infrastructure. Each failure, in it’s own unique way, can help us building a better version of our service. In the following section I want to present you with a few ways you can learn from failure.&lt;/p>
&lt;h3 id="unveil-hidden-weaknesses">Unveil hidden weaknesses&lt;/h3>
&lt;p>I don&amp;rsquo;t want to repeat myself and bore you, but once again: As we have learned from the &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a>, failures are what happens when the system behaves differently than we think it will behave. So each incident gauges the delta of our understanding. When our mental model of the system behavior gets better, we can eventually find bugs or procedural and structural failures in our code. This allows us to make the improvements needed to arrive at a more resilient system.&lt;/p>
&lt;h3 id="put-your-resilience-to-the-test">Put your resilience to the test&lt;/h3>
&lt;p>Each failure can be seen as an unscheduled drill for your system’s resilience mechanisms. Moments of crisis battle-test your redundancies, failover strategies, and even disaster recovery protocols. Chaos Experimentation is certainly the better approach to build confidence in your resilience mechanisms, but sometimes it takes a real incident to see how well every safeguard and every redundancy measure works hand-in-hand together with each other.&lt;/p>
&lt;h3 id="sharpen-your-incident-response">Sharpen your Incident Response&lt;/h3>
&lt;p>Failures are the real-world stage on which your carefully crafted play of incident response procedures are executed. You can reason about incident response processes as much as you want but there is nothing that tests the effectiveness of communication protocols as well as the teams collaboration and agility during identifying and mitigating a real incident when real money and real customer demand is on the line.
While tabletop exercises of incident drills are a good way to validate some of you processes, the critically and fire during a real production outage is the ultimate test. Each stumble and misstep during the incident response procedure is a chance to fine-tune your approach, ultimately minimizing the fallout and and optimization of your &lt;a href="https://dzone.com/articles/what-are-mttx-metrics-good-for-lets-find-out">TTx metrics&lt;/a> of future incidents.&lt;/p>
&lt;h3 id="guiding-future-design">Guiding future design&lt;/h3>
&lt;p>The wisdom derived from actual failures not only bolsters existing systems but also informs the design of future systems. Understanding the triggers of past failures equips engineers to preemptively design systems that sidesteps these pitfalls, inherently making them more robust and reliable.&lt;/p>
&lt;h2 id="strategies-and-techniques-for-dealing-with-failures">Strategies and techniques for dealing with failures&lt;/h2>
&lt;p>As mentioned many times throughout this article, incidents, while they are undesirable, are an integral part of our cloud infrastructure. They test our resilience to failure, challenge our readiness to respond, and drive us towards an ever-evolving state of improvement.
I wanna dive into a couple of topics showcasing how we can effectively respond to, recover from, and learn from incidents.&lt;/p>
&lt;h3 id="incident-response-processes">Incident Response processes&lt;/h3>
&lt;p>Once our infrastructure begins to fail it calls for an swift and structured response.
Effective incident response requires three major pillars:&lt;/p>
&lt;ol>
&lt;li>Fast Detection&lt;/li>
&lt;li>Rapid Mitigation&lt;/li>
&lt;li>Clear lines of communication&lt;/li>
&lt;/ol>
&lt;p>Lets break these three items down further:&lt;/p>
&lt;ol>
&lt;li>To allow for a fast detection we require solid monitoring of our infrastructure and proper instrumentation of our application.&lt;/li>
&lt;li>Once we discovered that something went wrong we have to focus on mitigating the issue.&lt;/li>
&lt;li>Clear lines of communication tremendously help in coordinating remediation measures across teams and dependent services.&lt;/li>
&lt;/ol>
&lt;p>Incident Management is a highly complex topic in it’s own and deserves it’s own blog-post in the future. Stay tuned.&lt;/p>
&lt;h3 id="effective-recovery">Effective Recovery&lt;/h3>
&lt;p>Recovering effectively again is a delicate topic and requires in-depth knowledge about the type of service and the specific infrastructure.
There is no “one-size-fits-all” solution to improve recovery time (often referred to [“TTM”][4] or [“Time to mitigate&amp;quot;][4]).&lt;/p>
&lt;p>However, one key concept that we can talk about here is “graceful degradation”. When designing complex, distributed, systems, we can design our system in a way, that even in the event of a failure of one sub-component or micro-serve, the system maintains functionality with reduced capacity or functionality and prevent a total system collapse.
Designing a complex system for graceful degradation can significantly limit the blast-radius of an incident.
However, you might have guessed it already: graceful degradation is a complex process in it’s own and there are entire &lt;a href="https://link.springer.com/book/10.1007/978-3-319-02429-5">books&lt;/a> written about designing distributed systems and handling failures.&lt;/p>
&lt;h3 id="post-incident-analysis">Post-Incident Analysis&lt;/h3>
&lt;p>Once the issue is under control and the incident is mitigated, we can get the post-incident analysis process started.
The &lt;a href="https://www.atlassian.com/incident-management/postmortem">post-mortem process&lt;/a> is where we can take time to reflect and learn from the failure and implement measures to prevent similar failures in the future.&lt;/p>
&lt;p>During the post-mortem analysis we try to understand the exact cause and progression of the incident. However we should refrain from finding a scapegoat. Post-mortem processes must be &lt;a href="https://youtu.be/emTzpdPgg7Q?t=1035">blameless&lt;/a> to be effective.
Every aspect of an incident, from it’s start to finish must be evaluated in great detail. This through examination provides insights into our system, the exact failure mode that occurred, it can give us some insights into how this failure mode can be prevented in the future and last but not least it highlights opportunities for improving the incident management process.&lt;/p>
&lt;h4 id="the-power-of-blameless-postmortem-culture">The power of blameless postmortem culture&lt;/h4>
&lt;p>A blameless postmortem, as the term suggests, is one where to focus is not on pointing fingers who did something wrong but to identify the conditions that led to the incident. In such an environment, team members feel free to provide honest insights without the fear of retribution. It is important that the human error is not the root-cause of a failure, but the result of systemic deficiencies.&lt;/p>
&lt;p>While the concept of blameless postmortem culture is gaining industry-wide recognition, another vital element of a healthy postmortem process is sanctionlessness. Sanctionless postmortems not only refrains from attributing blame but also from negative repercussions for those involved in the incident. It acknowledges that while humans make errors, these errors are merely symptoms of a systemic weakness.&lt;/p>
&lt;p>To explain this, let’s consider an incident caused by a junior engineer wo deployed faulty code.
n a blameless postmortem we would focus on understanding how the code made it through the review and was not caught during the QA processes and was allowed to progress through all of our release gates. Rather than simply blaming the junior engineer for pushing bad code we recognize the gaps in the process and the need for better testing procedures.
However, let’s assume following this postmortem management enforces new checklists, demanding more rigorous testing protocols and stricter gating of releases. Everyone who wants to deploy code changes now must comply with this vast list of action items ensuring the release is acceptable.
In this case, despite the postmortem being blameless, it wasn’t sanction-less. The negative consequences are that now everyone has to comply with overly long and dreadful checklists fostering a fear-driven culture, inhibiting honest communication since no-one wants to follow even tighter protocols.
Conversely, a truly sanction-less postmortem would dig deeper in why the already existing QA processes did not detect the issue earlier and would have allowed to gate this release. Maybe we would have found out that deadlines were too tight and engineers overworked and could not spend enough time on quality assurance. We could have identified that promises were made towards customers in regards to the availability of the feature creating time-pressure. This finding would allow us to re-think the communication strategy for new features to customers or the way we prioritize our feature backlog.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Building a resilient service involves accepting and navigating through the unpredictability of cloud infrastructure, recognizing failures as stepping stones to improve and even to utilize approaches such as chaos engineering to build confidence in your resiliency.
As we continue to operate our service, each failure should be viewed as an opportunity for learning and enhancing our systems. Thus, ensuring the resilience of our digital reality is not merely about combating unpredictability, but harnessing it to create stronger and more reliable services.&lt;/p></description></item><item><title>Site Reliability Engineering Explained: An Exploration of DevOps, Platform Engineering, and SRE</title><link>https://cedi.dev/talk/sre-explained/</link><pubDate>Sat, 10 Jun 2023 10:35:00 +0000</pubDate><guid>https://cedi.dev/talk/sre-explained/</guid><description/></item><item><title>Understanding Alerting - How to come up with a good enough alerting strategy</title><link>https://cedi.dev/talk/alerting/</link><pubDate>Fri, 20 May 2022 13:00:00 +0000</pubDate><guid>https://cedi.dev/talk/alerting/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Above-the-line / Below-the-line framework</title><link>https://cedi.dev/post/above-the-line-framework/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/above-the-line-framework/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Welcome back to my blog!&lt;/p>
&lt;p>In this article I want to challenge how you think about your systems design.
The above-the-line / below-the-line framework might appear weird at first, since it is introducing some level of abstraction between you and the system, but I hope while reading trough the article and reflecting on it, you come to terms with the framework.
If you accept it, it will allow you to write better documentation, communicate better about a system architecture, and come up with better monitoring for it.&lt;/p>
&lt;p>I hope you will find this article helpful! If you do - please consider sharing it to your co-workers and friends!&lt;/p>
&lt;h2 id="modelling-a-system">Modelling a system&lt;/h2>
&lt;p>When you are performing any kind of systems-design, you will produce diagrams such as the one below. I would argue, most readers of this are familiar with some sort or another of such diagrams and use them almost daily as part of their job.
A typical systems consists of software components running on various servers and clusters behind layers of load balancers and routers, using databases running in close proximity to the application, connecting to 3rd party APIs over the internet, and presenting results to other systems or users.
&lt;img src="images/system.png" alt="system">&lt;/p>
&lt;p>And there is absolutely nothing wrong with this systems design overview. Except maybe that it is on a very high level.&lt;/p>
&lt;p>But you need some amount of supporting infrastructure in order to run your designed system. Maybe you&amp;rsquo;re depending on a cloud-provider, but most certainly your application code and pipeline definitions exists in repositories. Pipelines are executed by pipeline runners on some other infrastructure. You have additional deployment tools, helper scripts, and so on.&lt;/p>
&lt;p>For simplicity let’s focus on the least amount of supporting infrastructure needed: Version Control Servers, Testing Tools, Pipelines, and issue-trackers.
I think we quickly end up with something like this:
&lt;img src="images/overview.png" alt="overview">&lt;/p>
&lt;p>Yay - now we successfully managed to map out the most important parts of our system. We have the system itself as well as the supporting infrastructure.&lt;/p>
&lt;p>Looks good, right?&lt;/p>
&lt;h2 id="interacting-with-a-system">Interacting with a system&lt;/h2>
&lt;p>Now let’s have a look at the different individuals involved in building, supporting, and using the system. These are the developers, SREs, QA engineers, and program managers.&lt;/p>
&lt;p>They all have different backgrounds and might have different intentions when interacting with the system. While the PMs want to push for new features, QA might push for fewer bugs and the SREs might push for higher reliability and resiliency.&lt;/p>
&lt;p>But how exactly are they going to interact with the system?&lt;/p>
&lt;p>We experience this every day in our jobs. The developers will write code that they push to the repositories in our VCS based on feature-requests of but-tickets in the ticket-system, the SREs will observe the systems state via the monitoring tools to inform decisions about improving reliability, DevOps might improve upon the CI/CD pipelines by editing the YAML files which they push to the VCS for the pipeline-system to pick them up. The QA engineers use the web-frontend of the deployment tool to kick-off a pipeline run deploying to a test-environment, perform their tests using a vast inventory of testing tools and test descriptions. And then there are our PMs who will write JIRA-Tickets to request new features.&lt;/p>
&lt;p>Sounds familiar, doesn&amp;rsquo;t it?
&lt;img src="images/people.png" alt="people">&lt;/p>
&lt;p>And everyone working in this area knows that there is a problem. The great disconnect between QA who thinks the developers can&amp;rsquo;t write bug-free code, the PMs who think the SREs are unreasonable to push back on a release on a Friday evening because the error budget is exhausted for the month … .&lt;/p>
&lt;p>After all, everyone here knows this meme and to some extend relate to it, am I right?:
&lt;img src="images/meme.jpg" alt="how project managers, developers, qa, sysadmin, designers see each other">&lt;/p>
&lt;p>But why is that?&lt;/p>
&lt;p>Because everyone understands the system differently. Everyone has a different mental-model of how the system works. And how it will fail.
No one has a complete understanding of the system.
No one knows exactly how it will fail.
Everyone has different experiences with how the system failed in the past, or how similar systems failed in the past.
This lays in the very nature of every only so slightly complex system. And systems in the area of computer-systems are always complex ones.&lt;/p>
&lt;p>&lt;img src="images/mentalmodels.png" alt="mental models">&lt;/p>
&lt;h2 id="the-_above-the-line--below-the-line_-framework">The &lt;em>above the line / below the line&lt;/em> framework&lt;/h2>
&lt;p>After looking at a system itself and the different people interacting with a system, it is time to combine the two into one model and call with the above the line / below the line framework.&lt;/p>
&lt;p>We call it that, because we draw a line and put the people interacting with the system and their different mental models and intentions above this line and our beloved complex system below this line.&lt;/p>
&lt;p>&lt;img src="images/above-the-line-below-the-line.png" alt="above-the-line/below-the-line">&lt;/p>
&lt;p>We call this (green) line the &lt;em>line of representation&lt;/em>, because whenever we interact with our system, we interact with a representation of our system.&lt;/p>
&lt;p>The consequence of this framework is, that everything &amp;ldquo;below the line&amp;rdquo; is interfered from the mental models of the individuals and therefore that the system itself does not exist in the physical world.
We cannot see, touch, or directly control our system.
The only way we can interact with our system is trough the representation layer.&lt;/p>
&lt;p>I think unconsciously we know this already.&lt;/p>
&lt;p>Why else would we rely on &amp;ldquo;infrastructure as code&amp;rdquo;? And why else would we rather commit our CI/CD Pipeline definition and configurations as YAML files than using a bulky UI (looking at you Jenkins)?
We choose a representation that is familiar to us to define our systems state to make it easier to work with.
It is easier for us to inform our mental model based on a known representation. And the other way round, it is easy to translate our mental-model into a representation that is familiar to us.&lt;/p>
&lt;p>One very important consequence of this model is, that every time someone makes any change to a system, the change is based on the persons mental model of the system at a certain point in time.&lt;/p>
&lt;p>But the data which informs our mental model becomes stale very quickly.
We know that code and configuration changes over time. The same is true for the requirements of our system.
Even the hardware on which system is running changes over time and become less reliable because of bit-rot, degrading optical transceivers, or other variables.
And finally 3rd party application behavior changes over time thanks to updates, causing a change in the access pattern of your drives or how it uses the network.&lt;/p>
&lt;p>We have to accept and live with a constantly changing system.&lt;/p>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>We can&amp;rsquo;t talk about this framework without talking about challenges that arise as a consequence of this system.&lt;/p>
&lt;p>I think the most important two are&lt;/p>
&lt;ol>
&lt;li>Every individual working with a system must develop and maintain a good-enough mental model of the system.&lt;/li>
&lt;li>Every individual working with a system must develop and maintain a good-enough understanding of other individuals mental-models of the system.&lt;/li>
&lt;/ol>
&lt;p>For new-hires we have the Onboarding-phase in which a new-hire is expected to learn everything about the system. During this phase we build our initial mental-model of the system.&lt;/p>
&lt;p>For everyone else we have knowledge-sharing-sessions or something like that.
We have architecture-deep-dive sessions with our peers, or conduct systems-design meetings when planning for big-changes.
And finally we review each-others merge requests.&lt;/p>
&lt;p>We don&amp;rsquo;t do all this for the sake of spotting someone else’s mistake.&lt;/p>
&lt;p>Instead we validate each others assumptions about how the system behaves and therefore refine each-others mental-models about the system.&lt;/p>
&lt;p>What might appears to us as a &amp;ldquo;design-flaw&amp;rdquo; or &amp;ldquo;error&amp;rdquo; in someone else’s design or code-change might not be because they &amp;ldquo;did a mistake&amp;rdquo; but because their mental-model of how the system works and or fails was flawed.&lt;/p>
&lt;p>As mentioned above:&lt;/p>
&lt;blockquote>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;/blockquote>
&lt;p>This is also true for reviews. If we are surprised by a suggestion to change the design or code, that is likely because our own mental-model isn’t aligned with the mental-model of the person which suggestion you are reviewing.&lt;/p>
&lt;p>In my opinion, frequent architecture or design reviews, as well as active participation in merge request reviews are crucial, because this helps us to inform each-other about our mental-models and it makes it easier to get a feeling and understanding of how others form their mental-model.&lt;/p></description></item></channel></rss>