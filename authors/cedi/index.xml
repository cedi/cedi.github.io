<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cedi.dev</title><link>https://cedi.dev/authors/cedi/</link><description>Recent content on cedi.dev</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} by Cedric Kienzler</copyright><atom:link href="https://cedi.dev/authors/cedi/index.xml" rel="self" type="application/rss+xml"/><item><title>Breaking Clouds</title><link>https://cedi.dev/post/breaking-clouds/</link><pubDate>Tue, 22 Aug 2023 20:00:00 +0000</pubDate><guid>https://cedi.dev/post/breaking-clouds/</guid><description>&lt;p>Cloud infrastructure is a necessity in our modern digital world. However, understanding and preparing for failures in cloud infrastructure is critical for reliability of our services. Failures can be viewed as learning opportunities and to improve our system design. It can inform proactive problem-solving, fostering effective incident response, and guiding future design challenges. &lt;a href="https://en.wikipedia.org/wiki/Chaos_engineering">Chaos Engineering&lt;/a> plays a vital role in testing for resilience of our system.&lt;/p>
&lt;h2 id="the-cloud-as-our-new-reality">The cloud as our new reality&lt;/h2>
&lt;p>Cloud Infrastructure has become an integral part of our modern digital landscape. It’s the foundation that supports a majority of the digital services used by millions of people every day. From social media platforms to online video conferencing enabling hybrid work. Even sectors like healthcare or education, cloud infrastructure plays a vital role these days.
Due to the cost-effectiveness and easy accessibility it has become easier and easier building more and more complex systems with ease and minimal knowledge required. Where once tens of infrastructure engineers maintained dozens of servers in a rack, nowadays any single developer can spin up vast infrastructure landscapes with intricate dependencies on public cloud-providers such as Azure or GCP.&lt;/p>
&lt;p>However this comes with the expectation of reliability and resilience. In this context, reliability means that cloud services should be available and function correctly when users need them.
Resilience means that these services can quickly recover from any disruption or failure.
Given our modern reality, a significant part of our everyday lives is now “in the cloud”. We rely on it through our entire live.
From communication both private and professional, the digital workplace, as well as entertainment, and even critical services such as healthcare set the expectations high when it comes to reliability and resilience.&lt;/p>
&lt;p>This is why understanding and preparing for potential failures in cloud infrastructure is so critical.&lt;/p>
&lt;h2 id="exploring-failures">Exploring failures&lt;/h2>
&lt;p>Failure modes, or commonly just called &amp;ldquo;failures&amp;rdquo;, describe the various ways in which a system, or service fails.
Failure modes are as diverse as the landscape of modern cloud infrastructure itself. They can range from Hardware to Software and even &lt;a href="https://youtu.be/Oev_PJm2yUQ?t=108">Human Failure&lt;/a>.
I think Hardware, Software failures are quite self explanatory: These involve the failure of hardware, like servers, routers, or storage devices, or bugs and glitches in the software that cause system malfunctions. But the effect is always the same: Users seeing errors, dropped connections, degraded system performance or intermittent problems of all sorts.
But to me, the most interesting failure modes are failures that involve the human factor. These are failures that happen when humans are involved in a process, like during incident response for example, and they make a mistake.
Human factors are responsible for an entire new spectrum of failure modes. From unplugging the wrong cable in the data center during a routine maintenance to configuring something that just does not work to executing the a destructive command in the wrong terminal window connected to the wrong server (Looking at you &lt;a href="https://about.gitlab.com/blog/2017/02/01/gitlab-dot-com-database-incident/#third-incident">GitLab&lt;/a>).&lt;/p>
&lt;p>By considering how individual components might fail we can develop a better design that is resilient to those failures.
We can use the &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a> to better understand the relationship between what the system actually is and how we think of it. We can remember that incidents inform us how the system actually behaves and how our assumptions about it where wrong. And gauging this delta enables us to take proactive measures to prevent such failures in the future, or at least make them less likely.
If you are a software developer you are probably familiar with the process of Pull Request reviews. They are a great tool and serve multiple functions that might not be obvious at first. The obvious benefit of Pull Request is that the code is screened to be bug free. But it also ensures that the mental model of the person who submits the pull request was correct and the correct assumptions were made when changing the code to not cause any unwanted side effects. And finally, it also serves as a vessel to update the mental model of the person who reviews the pull request by forcing that person to understand how the system behavior will change by this code change.&lt;/p>
&lt;p>Enough talking about Pull Requests and back to the topic of resiliency engineering. When we have an understanding of how a system might fail, we can take better informed decisions and have streamlined incident response processes in place help speeding up recovery. It enables engineers to more quickly diagnose and address the root cause of an incident with less friction.&lt;/p>
&lt;h2 id="failures-as-learning-opportunities">Failures as Learning Opportunities&lt;/h2>
&lt;p>While system failure is often viewed as a negative, it’s time to challenge this perspective. Instead we should consider these disruptions as an opportunity to improve our own mental model about how the system works (see &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a>) and inform our decisions going forward helping us to improve our infrastructure. Each failure, in it’s own unique way, can help us building a better version of our service. In the following section I want to present you with a few ways you can learn from failure.&lt;/p>
&lt;h3 id="unveil-hidden-weaknesses">Unveil hidden weaknesses&lt;/h3>
&lt;p>I don&amp;rsquo;t want to repeat myself and bore you, but once again: As we have learned from the &lt;a href="post/above-the-line-framework/">Above the line/Below the line framework&lt;/a>, failures are what happens when the system behaves differently than we think it will behave. So each incident gauges the delta of our understanding. When our mental model of the system behavior gets better, we can eventually find bugs or procedural and structural failures in our code. This allows us to make the improvements needed to arrive at a more resilient system.&lt;/p>
&lt;h3 id="put-your-resilience-to-the-test">Put your resilience to the test&lt;/h3>
&lt;p>Each failure can be seen as an unscheduled drill for your system’s resilience mechanisms. Moments of crisis battle-test your redundancies, failover strategies, and even disaster recovery protocols. Chaos Experimentation is certainly the better approach to build confidence in your resilience mechanisms, but sometimes it takes a real incident to see how well every safeguard and every redundancy measure works hand-in-hand together with each other.&lt;/p>
&lt;h3 id="sharpen-your-incident-response">Sharpen your Incident Response&lt;/h3>
&lt;p>Failures are the real-world stage on which your carefully crafted play of incident response procedures are executed. You can reason about incident response processes as much as you want but there is nothing that tests the effectiveness of communication protocols as well as the teams collaboration and agility during identifying and mitigating a real incident when real money and real customer demand is on the line.
While tabletop exercises of incident drills are a good way to validate some of you processes, the critically and fire during a real production outage is the ultimate test. Each stumble and misstep during the incident response procedure is a chance to fine-tune your approach, ultimately minimizing the fallout and and optimization of your &lt;a href="https://dzone.com/articles/what-are-mttx-metrics-good-for-lets-find-out">TTx metrics&lt;/a> of future incidents.&lt;/p>
&lt;h3 id="guiding-future-design">Guiding future design&lt;/h3>
&lt;p>The wisdom derived from actual failures not only bolsters existing systems but also informs the design of future systems. Understanding the triggers of past failures equips engineers to preemptively design systems that sidesteps these pitfalls, inherently making them more robust and reliable.&lt;/p>
&lt;h2 id="strategies-and-techniques-for-dealing-with-failures">Strategies and techniques for dealing with failures&lt;/h2>
&lt;p>As mentioned many times throughout this article, incidents, while they are undesirable, are an integral part of our cloud infrastructure. They test our resilience to failure, challenge our readiness to respond, and drive us towards an ever-evolving state of improvement.
I wanna dive into a couple of topics showcasing how we can effectively respond to, recover from, and learn from incidents.&lt;/p>
&lt;h3 id="incident-response-processes">Incident Response processes&lt;/h3>
&lt;p>Once our infrastructure begins to fail it calls for an swift and structured response.
Effective incident response requires three major pillars:&lt;/p>
&lt;ol>
&lt;li>Fast Detection&lt;/li>
&lt;li>Rapid Mitigation&lt;/li>
&lt;li>Clear lines of communication&lt;/li>
&lt;/ol>
&lt;p>Lets break these three items down further:&lt;/p>
&lt;ol>
&lt;li>To allow for a fast detection we require solid monitoring of our infrastructure and proper instrumentation of our application.&lt;/li>
&lt;li>Once we discovered that something went wrong we have to focus on mitigating the issue.&lt;/li>
&lt;li>Clear lines of communication tremendously help in coordinating remediation measures across teams and dependent services.&lt;/li>
&lt;/ol>
&lt;p>Incident Management is a highly complex topic in it’s own and deserves it’s own blog-post in the future. Stay tuned.&lt;/p>
&lt;h3 id="effective-recovery">Effective Recovery&lt;/h3>
&lt;p>Recovering effectively again is a delicate topic and requires in-depth knowledge about the type of service and the specific infrastructure.
There is no “one-size-fits-all” solution to improve recovery time (often referred to [“TTM”][4] or [“Time to mitigate&amp;quot;][4]).&lt;/p>
&lt;p>However, one key concept that we can talk about here is “graceful degradation”. When designing complex, distributed, systems, we can design our system in a way, that even in the event of a failure of one sub-component or micro-serve, the system maintains functionality with reduced capacity or functionality and prevent a total system collapse.
Designing a complex system for graceful degradation can significantly limit the blast-radius of an incident.
However, you might have guessed it already: graceful degradation is a complex process in it’s own and there are entire &lt;a href="https://link.springer.com/book/10.1007/978-3-319-02429-5">books&lt;/a> written about designing distributed systems and handling failures.&lt;/p>
&lt;h3 id="post-incident-analysis">Post-Incident Analysis&lt;/h3>
&lt;p>Once the issue is under control and the incident is mitigated, we can get the post-incident analysis process started.
The &lt;a href="https://www.atlassian.com/incident-management/postmortem">post-mortem process&lt;/a> is where we can take time to reflect and learn from the failure and implement measures to prevent similar failures in the future.&lt;/p>
&lt;p>During the post-mortem analysis we try to understand the exact cause and progression of the incident. However we should refrain from finding a scapegoat. Post-mortem processes must be &lt;a href="https://youtu.be/emTzpdPgg7Q?t=1035">blameless&lt;/a> to be effective.
Every aspect of an incident, from it’s start to finish must be evaluated in great detail. This through examination provides insights into our system, the exact failure mode that occurred, it can give us some insights into how this failure mode can be prevented in the future and last but not least it highlights opportunities for improving the incident management process.&lt;/p>
&lt;h4 id="the-power-of-blameless-postmortem-culture">The power of blameless postmortem culture&lt;/h4>
&lt;p>A blameless postmortem, as the term suggests, is one where to focus is not on pointing fingers who did something wrong but to identify the conditions that led to the incident. In such an environment, team members feel free to provide honest insights without the fear of retribution. It is important that the human error is not the root-cause of a failure, but the result of systemic deficiencies.&lt;/p>
&lt;p>While the concept of blameless postmortem culture is gaining industry-wide recognition, another vital element of a healthy postmortem process is sanctionlessness. Sanctionless postmortems not only refrains from attributing blame but also from negative repercussions for those involved in the incident. It acknowledges that while humans make errors, these errors are merely symptoms of a systemic weakness.&lt;/p>
&lt;p>To explain this, let’s consider an incident caused by a junior engineer wo deployed faulty code.
n a blameless postmortem we would focus on understanding how the code made it through the review and was not caught during the QA processes and was allowed to progress through all of our release gates. Rather than simply blaming the junior engineer for pushing bad code we recognize the gaps in the process and the need for better testing procedures.
However, let’s assume following this postmortem management enforces new checklists, demanding more rigorous testing protocols and stricter gating of releases. Everyone who wants to deploy code changes now must comply with this vast list of action items ensuring the release is acceptable.
In this case, despite the postmortem being blameless, it wasn’t sanction-less. The negative consequences are that now everyone has to comply with overly long and dreadful checklists fostering a fear-driven culture, inhibiting honest communication since no-one wants to follow even tighter protocols.
Conversely, a truly sanction-less postmortem would dig deeper in why the already existing QA processes did not detect the issue earlier and would have allowed to gate this release. Maybe we would have found out that deadlines were too tight and engineers overworked and could not spend enough time on quality assurance. We could have identified that promises were made towards customers in regards to the availability of the feature creating time-pressure. This finding would allow us to re-think the communication strategy for new features to customers or the way we prioritize our feature backlog.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Building a resilient service involves accepting and navigating through the unpredictability of cloud infrastructure, recognizing failures as stepping stones to improve and even to utilize approaches such as chaos engineering to build confidence in your resiliency.
As we continue to operate our service, each failure should be viewed as an opportunity for learning and enhancing our systems. Thus, ensuring the resilience of our digital reality is not merely about combating unpredictability, but harnessing it to create stronger and more reliable services.&lt;/p></description></item><item><title>From 0 to Kubernetes. An introduction to container orchestration with practical answers to common questions like "why?" Or "when?"</title><link>https://cedi.dev/talk/from-zero-to-k8s-en/</link><pubDate>Sat, 05 Aug 2023 13:00:00 +0000</pubDate><guid>https://cedi.dev/talk/from-zero-to-k8s-en/</guid><description/></item><item><title>Modern Observability - Scalable Observability with the LGTM Stack: Harnessing the Power of Loki, Grafana, Tempo, and Mimir</title><link>https://cedi.dev/talk/modern-o11y/</link><pubDate>Sat, 10 Jun 2023 17:15:00 +0000</pubDate><guid>https://cedi.dev/talk/modern-o11y/</guid><description/></item><item><title>Site Reliability Engineering Explained: An Exploration of DevOps, Platform Engineering, and SRE</title><link>https://cedi.dev/talk/sre-explained/</link><pubDate>Sat, 10 Jun 2023 10:35:00 +0000</pubDate><guid>https://cedi.dev/talk/sre-explained/</guid><description/></item><item><title>From 0 to Kubernetes. Eine Einführung zur Container-Orchestrierung mit praktischen Antworten auf die häufigsten Fragen wie “warum?” Oder “wann?”</title><link>https://cedi.dev/talk/from-zero-to-k8s/</link><pubDate>Fri, 09 Jun 2023 13:50:00 +0000</pubDate><guid>https://cedi.dev/talk/from-zero-to-k8s/</guid><description/></item><item><title>Hilfe, ich bin Manager - und jetzt? Oder auch: Wie man in der IT Karriere machen kann.</title><link>https://cedi.dev/talk/denog14/</link><pubDate>Mon, 14 Nov 2022 10:00:00 +0000</pubDate><guid>https://cedi.dev/talk/denog14/</guid><description>&lt;!-- raw HTML omitted -->
&lt;p>Diese Panel Diskussion entstand in enger zusammenarbeit mit &lt;a href="https://www.linkedin.com/in/falk-stern-6046a060">Falk Stern&lt;/a> und &lt;a href="https://www.linkedin.com/in/annika-wickert-009a2ab4/">Annika Wickert&lt;/a>.&lt;/p></description></item><item><title>Kubernetes - The good, the bad, the ugly</title><link>https://cedi.dev/talk/kubernetes-good-bad-ugly/</link><pubDate>Fri, 20 May 2022 18:45:00 +0000</pubDate><guid>https://cedi.dev/talk/kubernetes-good-bad-ugly/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Understanding Alerting - How to come up with a good enough alerting strategy</title><link>https://cedi.dev/talk/alerting/</link><pubDate>Fri, 20 May 2022 13:00:00 +0000</pubDate><guid>https://cedi.dev/talk/alerting/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Above-the-line / Below-the-line framework</title><link>https://cedi.dev/post/above-the-line-framework/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/above-the-line-framework/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Welcome back to my blog!&lt;/p>
&lt;p>In this article I want to challenge how you think about your systems design.
The above-the-line / below-the-line framework was famously created, published, and popularised by &lt;a href="https://en.wikipedia.org/wiki/Richard_Cook_(safety_researcher)">Dr. Richard Cook&lt;/a> in the &lt;a href="https://snafucatchers.github.io/#2_3_The_above-the-line/below-the-line_framework">SNAFUcatchers Stella Report&lt;/a>. It might appear weird at first, since it is introducing some level of abstraction between you and the system, but I hope while reading trough the article and reflecting on it, you come to terms with the framework.
If you accept it, it will allow you to write better documentation, communicate better about a system architecture, and come up with better monitoring for it.&lt;/p>
&lt;p>I hope you will find this article helpful! If you do - please consider sharing it to your co-workers and friends!&lt;/p>
&lt;h2 id="modelling-a-system">Modelling a system&lt;/h2>
&lt;p>When you are performing any kind of systems-design, you will produce diagrams such as the one below. I would argue, most readers of this are familiar with some sort or another of such diagrams and use them almost daily as part of their job.
A typical systems consists of software components running on various servers and clusters behind layers of load balancers and routers, using databases running in close proximity to the application, connecting to 3rd party APIs over the internet, and presenting results to other systems or users.
&lt;img src="images/system.png" alt="system">&lt;/p>
&lt;p>And there is absolutely nothing wrong with this systems design overview. Except maybe that it is on a very high level.&lt;/p>
&lt;p>But you need some amount of supporting infrastructure in order to run your designed system. Maybe you&amp;rsquo;re depending on a cloud-provider, but most certainly your application code and pipeline definitions exists in repositories. Pipelines are executed by pipeline runners on some other infrastructure. You have additional deployment tools, helper scripts, and so on.&lt;/p>
&lt;p>For simplicity let’s focus on the least amount of supporting infrastructure needed: Version Control Servers, Testing Tools, Pipelines, and issue-trackers.
I think we quickly end up with something like this:
&lt;img src="images/overview.png" alt="overview">&lt;/p>
&lt;p>Yay - now we successfully managed to map out the most important parts of our system. We have the system itself as well as the supporting infrastructure.&lt;/p>
&lt;p>Looks good, right?&lt;/p>
&lt;h2 id="interacting-with-a-system">Interacting with a system&lt;/h2>
&lt;p>Now let’s have a look at the different individuals involved in building, supporting, and using the system. These are the developers, SREs, QA engineers, and program managers.&lt;/p>
&lt;p>They all have different backgrounds and might have different intentions when interacting with the system. While the PMs want to push for new features, QA might push for fewer bugs and the SREs might push for higher reliability and resiliency.&lt;/p>
&lt;p>But how exactly are they going to interact with the system?&lt;/p>
&lt;p>We experience this every day in our jobs. The developers will write code that they push to the repositories in our VCS based on feature-requests of but-tickets in the ticket-system, the SREs will observe the systems state via the monitoring tools to inform decisions about improving reliability, DevOps might improve upon the CI/CD pipelines by editing the YAML files which they push to the VCS for the pipeline-system to pick them up. The QA engineers use the web-frontend of the deployment tool to kick-off a pipeline run deploying to a test-environment, perform their tests using a vast inventory of testing tools and test descriptions. And then there are our PMs who will write JIRA-Tickets to request new features.&lt;/p>
&lt;p>Sounds familiar, doesn&amp;rsquo;t it?
&lt;img src="images/people.png" alt="people">&lt;/p>
&lt;p>And everyone working in this area knows that there is a problem. The great disconnect between QA who thinks the developers can&amp;rsquo;t write bug-free code, the PMs who think the SREs are unreasonable to push back on a release on a Friday evening because the error budget is exhausted for the month … .&lt;/p>
&lt;p>After all, everyone here knows this meme and to some extend relate to it, am I right?:
&lt;img src="images/meme.jpg" alt="how project managers, developers, qa, sysadmin, designers see each other">&lt;/p>
&lt;p>But why is that?&lt;/p>
&lt;p>Because everyone understands the system differently. Everyone has a different mental-model of how the system works. And how it will fail.
No one has a complete understanding of the system.
No one knows exactly how it will fail.
Everyone has different experiences with how the system failed in the past, or how similar systems failed in the past.
This lays in the very nature of every only so slightly complex system. And systems in the area of computer-systems are always complex ones.&lt;/p>
&lt;p>&lt;img src="images/mentalmodels.png" alt="mental models">&lt;/p>
&lt;h2 id="the-_above-the-line--below-the-line_-framework">The &lt;em>above the line / below the line&lt;/em> framework&lt;/h2>
&lt;p>After looking at a system itself and the different people interacting with a system, it is time to combine the two into one model and call with the above the line / below the line framework.&lt;/p>
&lt;p>We call it that, because we draw a line and put the people interacting with the system and their different mental models and intentions above this line and our beloved complex system below this line.&lt;/p>
&lt;p>&lt;img src="images/above-the-line-below-the-line.png" alt="above-the-line/below-the-line">&lt;/p>
&lt;p>We call this (green) line the &lt;em>line of representation&lt;/em>, because whenever we interact with our system, we interact with a representation of our system.&lt;/p>
&lt;p>The consequence of this framework is, that everything &amp;ldquo;below the line&amp;rdquo; is interfered from the mental models of the individuals and therefore that the system itself does not exist in the physical world.
We cannot see, touch, or directly control our system.
The only way we can interact with our system is trough the representation layer.&lt;/p>
&lt;p>I think unconsciously we know this already.&lt;/p>
&lt;p>Why else would we rely on &amp;ldquo;infrastructure as code&amp;rdquo;? And why else would we rather commit our CI/CD Pipeline definition and configurations as YAML files than using a bulky UI (looking at you Jenkins)?
We choose a representation that is familiar to us to define our systems state to make it easier to work with.
It is easier for us to inform our mental model based on a known representation. And the other way round, it is easy to translate our mental-model into a representation that is familiar to us.&lt;/p>
&lt;p>One very important consequence of this model is, that every time someone makes any change to a system, the change is based on the persons mental model of the system at a certain point in time.&lt;/p>
&lt;p>But the data which informs our mental model becomes stale very quickly.
We know that code and configuration changes over time. The same is true for the requirements of our system.
Even the hardware on which system is running changes over time and become less reliable because of bit-rot, degrading optical transceivers, or other variables.
And finally 3rd party application behavior changes over time thanks to updates, causing a change in the access pattern of your drives or how it uses the network.&lt;/p>
&lt;p>We have to accept and live with a constantly changing system.&lt;/p>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>We can&amp;rsquo;t talk about this framework without talking about challenges that arise as a consequence of this system.&lt;/p>
&lt;p>I think the most important two are&lt;/p>
&lt;ol>
&lt;li>Every individual working with a system must develop and maintain a good-enough mental model of the system.&lt;/li>
&lt;li>Every individual working with a system must develop and maintain a good-enough understanding of other individuals mental-models of the system.&lt;/li>
&lt;/ol>
&lt;p>For new-hires we have the Onboarding-phase in which a new-hire is expected to learn everything about the system. During this phase we build our initial mental-model of the system.&lt;/p>
&lt;p>For everyone else we have knowledge-sharing-sessions or something like that.
We have architecture-deep-dive sessions with our peers, or conduct systems-design meetings when planning for big-changes.
And finally we review each-others merge requests.&lt;/p>
&lt;p>We don&amp;rsquo;t do all this for the sake of spotting someone else’s mistake.&lt;/p>
&lt;p>Instead we validate each others assumptions about how the system behaves and therefore refine each-others mental-models about the system.&lt;/p>
&lt;p>What might appears to us as a &amp;ldquo;design-flaw&amp;rdquo; or &amp;ldquo;error&amp;rdquo; in someone else’s design or code-change might not be because they &amp;ldquo;did a mistake&amp;rdquo; but because their mental-model of how the system works and or fails was flawed.&lt;/p>
&lt;p>As mentioned above:&lt;/p>
&lt;blockquote>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;/blockquote>
&lt;p>This is also true for reviews. If we are surprised by a suggestion to change the design or code, that is likely because our own mental-model isn’t aligned with the mental-model of the person which suggestion you are reviewing.&lt;/p>
&lt;p>In my opinion, frequent architecture or design reviews, as well as active participation in merge request reviews are crucial, because this helps us to inform each-other about our mental-models and it makes it easier to get a feeling and understanding of how others form their mental-model.&lt;/p>
&lt;h2 id="attribution">Attribution&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Richard_Cook_(safety_researcher)">Dr. Richard Cook&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://snafucatchers.github.io/#2_3_The_above-the-line/below-the-line_framework">SNAFUcatchers Stella Report&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://queue.acm.org/detail.cfm?id=3380777">Above the Line, Below the Line. The resilience of Internet-facing systems relies on what is below the line of representation.&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>What the heck are devcontainers?</title><link>https://cedi.dev/post/devcontainer-pt1/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/devcontainer-pt1/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hi and welcome back to my blog.&lt;/p>
&lt;p>I think it&amp;rsquo;s about time to talk about an amazing feature of VScode that I rarely see used in the wild. And honestly: I do not understand why few are using it.
You might have guessed it from the title. The feature is called &amp;ldquo;devcontainers&amp;rdquo;.&lt;/p>
&lt;p>But you might ask: &amp;ldquo;What are devcontainers?&amp;rdquo; and &amp;ldquo;Why should I care?&amp;rdquo;.&lt;/p>
&lt;p>I&amp;rsquo;m about to explain devcontainers in a small series of blog-posts, starting with this one.&lt;/p>
&lt;p>In this post I am going to talk about what devcontainers are, why you should care, and what the advantage of devcontainers are.
After reading this I hope you want to add a devcontainer to every Git Repository you work with.&lt;/p>
&lt;p>In a second part, I will continue our journey with devcontainers and talk about how to set-up a devcontainer and what resources are available to you when creating them.&lt;/p>
&lt;h2 id="what-are-devcontainers">What are devcontainers?&lt;/h2>
&lt;p>Technically, devcontainers are ordinary (Docker) containers.
But it&amp;rsquo;s not &lt;em>any&lt;/em> Docker container. It&amp;rsquo;s a Docker container that is you virtualised development environment. You install all of your development tools (like compilers, code generators, doc generators, and so on) in the devcontainer instead of installing all that on your local machine.&lt;/p>
&lt;p>And using devcontainers (using the &lt;code>devcontainer.json&lt;/code> config) you can even configure VScode. But not only configuration, the configuration can (and should) specify the extensions that will be automatically installed.&lt;/p>
&lt;p>And the nice thing: If you have the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">&lt;code>ms-vscode-remote.remote-containers&lt;/code>&lt;/a> extension installed and you open a folder in VScode that has a &lt;code>.devcontainer&lt;/code> folder inside of it, and that folder contains a &lt;code>devcontainer.json&lt;/code> and a &lt;code>Dockerfile&lt;/code> VScode will ask you if you want to reopen VScode in the devcontainer:&lt;/p>
&lt;p>&lt;img src="images/devcontainer-reopen.png" alt="devcontainer-reopen">&lt;/p>
&lt;p>If it doesn&amp;rsquo;t prompt you, you can select it from the command palette:&lt;/p>
&lt;p>&lt;img src="images/reopen-command.png" alt="devcontainer-reopen">&lt;/p>
&lt;p>You can give it a try right now. The &lt;a href="https://github.com/cedi/cedi.github.io/">GitHub Repository&lt;/a> hosting this blog comes with a &lt;a href="https://github.com/cedi/cedi.github.io/tree/main/.devcontainer">devcontainer&lt;/a>.&lt;/p>
&lt;h2 id="why-should-you-care">Why should you care?&lt;/h2>
&lt;p>If you&amp;rsquo;re anything like me, it&amp;rsquo;s likely you work with a multitude of programming languages, frameworks, and tools.
You have dozens of repositories checked out and about enough VScode plugins installed, the VScode startup time is comparable to the startup time of VisualStudio 2022 Ultimate Edition.&lt;/p>
&lt;p>But it doesn&amp;rsquo;t have to be like this.
With Devcontainers you have the bare minimum VScode installation with the only little configuration or plugins. Startup times are blazingly fast.&lt;/p>
&lt;p>You don&amp;rsquo;t have to worry about that ruby installation that you forgotten about because you tried to get Jekyll running before finally giving up and switching to Hugo.
You don&amp;rsquo;t have to care if you have one codebase uses .NET 4 while the other codebase uses .NET Core 5.
Because that is all taken care of inside of you devcontainer.&lt;/p>
&lt;p>Imagine you&amp;rsquo;re a new-hire in your company or a vendor joining a new project, you have to install a dozen different compilers, frameworks, libraries, tools to test, and tools to refactor your code. How long does it take you to get everything setup and being ready to get started? A day, or two?
Imagine you&amp;rsquo;re the Manager of a team and every new-hire or new contractor requires multiple days to get his workstation set-up.&lt;/p>
&lt;p>The time wasted with setting up your development environment is almost unbearable. But the solution is simple enough: Put the entire development environment inside of a devcontainer.&lt;/p>
&lt;p>The entire setup process would be installing&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://git-scm.com/downloads">Git&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/get-docker/">Docker Desktop&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://code.visualstudio.com/download">VScode&lt;/a>&lt;/li>
&lt;li>The &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">&lt;code>ms-vscode-remote.remote-containers&lt;/code>&lt;/a> extension&lt;/li>
&lt;li>Clone the repository&lt;/li>
&lt;li>open it in VScode&lt;/li>
&lt;/ol>
&lt;p>and boom — you are done!&lt;/p>
&lt;p>Wouldn&amp;rsquo;t that be amazing?&lt;/p>
&lt;p>Stay tuned for part 2, when we talk about how to set-up a devcontainer.&lt;/p>
&lt;h2 id="codespaces">Codespaces&lt;/h2>
&lt;p>The last reason for using Devcontainers is &lt;a href="https://github.com/features/codespaces">Codespaces&lt;/a>.
If you never heard of Codespaces before, here is a quick summary from the &lt;a href="https://github.com/features/codespaces">GitHub Codespaces page&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Use the full power of Visual Studio Code, including the editor, terminal, debugger, version control, settings sync, and the entire ecosystem of extensions. Work in the browser or hand off to your desktop.&lt;/p>
&lt;p>Spin up new dev environment for any sized project in seconds with prebuilt images. GitHub’s own 35GB dev image starts in under 10 seconds. Scale your cloud VMs up to 32 cores and 64GB of RAM. And with low-latency connections across four regions, you won’t even remember it’s not your local machine.&lt;/p>
&lt;/blockquote>
&lt;p>If your repository contains a Devcontainer and you’re hosting your Repository on GitHub, you can make use of Codespaces without any further setup required.
No need to install anything on your local machine anymore — heck, you could even code from your iPad.&lt;/p></description></item><item><title>Bootstrapping a production ready Kubernetes on Hetzner Cloud</title><link>https://cedi.dev/post/prod-ready-kubeone/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/prod-ready-kubeone/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hi and welcome to my first blog post on my new website.&lt;/p>
&lt;p>I know what you are thinking right now &amp;ldquo;Oh no! Not another blogpost about setting up a Kubernetes Cluster!&amp;rdquo;.
And yeah, I get it! There are a lot of blog-posts, tutorials, and articles already written about this topic.
For example &lt;a href="https://shibumi.dev">shibumi&lt;/a> wrote an amazing blog-post about &lt;a href="https://shibumi.dev/posts/kubernetes-on-hetzner-in-2021">Kubernetes on Hetzner in 2021&lt;/a>, and there is a even a &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/hetzner">Hetzner example terraform&lt;/a> in the &lt;a href="https://github.com/kubermatic/kubeone">KubeOne GitHub&lt;/a>.&lt;/p>
&lt;p>But here is the deal: while those posts and examples give you an &lt;em>easy&lt;/em> quick-start, they don&amp;rsquo;t cover the aspects of bootstrapping a KubeOne cluster that is supposed to run in production some day.&lt;/p>
&lt;p>To scope this blog-post a bit down, I have to make a few assumptions:&lt;/p>
&lt;ul>
&lt;li>You already have KubeOne installed on your local machine&lt;/li>
&lt;li>You have a project on Hetzner Cloud&lt;/li>
&lt;li>You have already created a Hetzner API Token for your Project with read+write permissions, or you are able to do so&lt;/li>
&lt;li>You want to use &lt;strong>Canal&lt;/strong> as your CNI of choice and not Cilium which was recently added in &lt;a href="https://github.com/kubermatic/kubeone/releases/tag/v1.4.0">KubeOne 1.4&lt;/a>.&lt;/li>
&lt;li>You are familiar with &lt;a href="https://www.terraform.io">terraform&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="preface-and-acknowledgements">Preface and Acknowledgements&lt;/h2>
&lt;p>This is not a &amp;ldquo;definitive guide&amp;rdquo; nor should you take everything you read too serious. I might be wrong or too opinionated about stuff.&lt;/p>
&lt;p>Actually running Kubernetes in production is way harder than just reading this article. I intentionally leave out &lt;strong>many&lt;/strong> details of how to actually run Kubernetes in production.&lt;/p>
&lt;p>Specifically, in this post I will not talk about:&lt;/p>
&lt;ul>
&lt;li>Security&lt;/li>
&lt;li>GitOps&lt;/li>
&lt;li>Disaster recovery&lt;/li>
&lt;li>Monitoring here&lt;/li>
&lt;/ul>
&lt;p>I might dedicate future blog-posts to those topics (and hopefully remember to link them back here).&lt;/p>
&lt;p>Therefore, the scope of this blog post is narrowed down to bootstrapping a Kubernetes Cluster using KubeOne - with somewhat sane defaults and measures taken - that will get you started on your journey to a production ready Kubernetes Cluster.&lt;/p>
&lt;h2 id="reliability-tip-1-use-an-odd-number-of-api-servers">Reliability Tip 1: Use an odd number of API servers&lt;/h2>
&lt;p>To get started we need to have some virtual servers running on Hetzner Cloud to install the Kubernetes API Server, etcd database and the cluster&amp;rsquo;s control-plane on.
You should stick to odd numbers of your API servers because etcd needs a majority of nodes to agree on updates to the cluster state.&lt;/p>
&lt;p>This majority (quorum) required for etcd is &lt;code>(n/2)+1&lt;/code> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The &lt;a href="https://etcd.io/docs/v3.3/faq/#why-an-odd-number-of-cluster-members">etcd FAQ&lt;/a> page describes it:&lt;/p>
&lt;blockquote>
&lt;p>For any odd-sized cluster, adding one node will always increase the number of nodes necessary for quorum. Although adding a node to an odd-sized cluster appears better since there are more machines, the fault tolerance is worse since exactly the same number of nodes may fail without losing quorum but there are more nodes that can fail. If the cluster is in a state where it can’t tolerate any more failures, adding a node before removing nodes is dangerous because if the new node fails to register with the cluster (e.g., the address is misconfigured), quorum will be permanently lost.&lt;/p>
&lt;/blockquote>
&lt;h2 id="reliability-tip-2-dont-use-the-count-meta-argument-of-terraform">Reliability Tip 2: Don&amp;rsquo;t use the &lt;code>count&lt;/code> meta-argument of terraform&lt;/h2>
&lt;p>Fortunate for us, KubeOne comes with a great set of &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/">examples&lt;/a> for using terraform to set up your infrastructure. We will use the &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/hetzner">hetzner example&lt;/a> as a base and customize it a bit.
It comes with mostly sane defaults and best practices out of the box, including a firewall and a &lt;a href="https://docs.hetzner.com/cloud/placement-groups/overview">placement group&lt;/a> for our control-plane nodes to ensure higher reliability.&lt;/p>
&lt;p>But there is a problem with this example: The usage of the &lt;a href="https://www.terraform.io/language/meta-arguments/count">&lt;code>count&lt;/code> meta-argument&lt;/a> for the &lt;a href="https://github.com/kubermatic/kubeone/blob/bbdceaff5ab1d3360f7455ab5f81dbfde73bf161/examples/terraform/hetzner/main.tf#L111">hcloud_server&lt;/a> definition.&lt;/p>
&lt;p>At first this seems absolutely valid and an easy fix for avoiding duplicate code. But the devil lies within the details as you&amp;rsquo;re about to find out.&lt;/p>
&lt;p>Let&amp;rsquo;s say we want to change the location of our servers, update the base images of our servers, or add/remove an SSH key.
All those changes got something in common:
They will absolutely destroy and re-create the server.&lt;/p>
&lt;p>But because we are using the &lt;code>count&lt;/code> meta-argument, we cannot update (re-create) only one server at a time. We can only replace all servers simultanously.&lt;/p>
&lt;p>KubeOne deploys the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">etcd&lt;/a> database on the API servers, which means if we loose all three API server nodes at the same time, we will loose all three etcd database replicas as well. And if we loose etcd, we loose our entire cluster.&lt;/p>
&lt;p>Therefore, we need to replace everything with a &lt;code>count&lt;/code> meta-argument with an explicit object.
Yes, this causes code duplication.
But it allows us to upgrade one server at a time. And after every server update, we can re-run &lt;code>kubeone&lt;/code> to repair (or &lt;em>reconcile&lt;/em>) our cluster. By doing so, we perform a &lt;em>rolling update&lt;/em> of our control plane without loosing any data.&lt;/p>
&lt;p>Now, lets get to it. We have to change the code blocks in lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L95-L99">95-99&lt;/a>, lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L110-L126">110-126&lt;/a>, and lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L144-L154">144-154&lt;/a>.&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
main.tf before
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane&amp;#34; {
count = var.control_plane_replicas
server_id = element(hcloud_server.control_plane.*.id, count.index)
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane&amp;#34; {
count = var.control_plane_replicas
name = &amp;#34;${var.cluster_name}-control-plane-${count.index + 1}&amp;#34;
server_type = var.control_plane_type
image = var.image
location = var.datacenter
placement_group_id = hcloud_placement_group.control_plane.id
ssh_keys = [
hcloud_ssh_key.kubeone.id,
]
labels = {
&amp;#34;kubeone_cluster_name&amp;#34; = var.cluster_name
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target&amp;#34; {
count = var.control_plane_replicas
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = element(hcloud_server.control_plane.*.id, count.index)
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane,
hcloud_load_balancer_network.load_balancer
]
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>We have to remove the &lt;code>count&lt;/code> meta-argument, get rid of the &lt;code>element(..., count.index)&lt;/code> syntax and replace everything with actual references to explicit objects, so it looks like this:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
main.tf after
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_1&amp;#34; {
name = &amp;#34;api-1&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_1&amp;#34; {
server_id = hcloud_server.control_plane_1.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp1&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_1.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_1,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_1
]
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_2&amp;#34; {
name = &amp;#34;api-2&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_2&amp;#34; {
server_id = hcloud_server.control_plane_2.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp2&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_2.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_2,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_2
]
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_3&amp;#34; {
name = &amp;#34;api-3&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_3&amp;#34; {
server_id = hcloud_server.control_plane_3.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp3&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_3.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_3,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_3
]
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>As pointed out by &lt;a href="https://github.com/EarthlingDavey">EarthlingDavey&lt;/a> in &lt;a href="https://github.com/cedi/cedi.github.io/issues/20">#20&lt;/a> this also has some implications for the output.tf, requiring us to remove the &lt;code>count&lt;/code> meta-argument there as well.&lt;/p>
&lt;p>We must up first fix the &lt;code>ssh_command&lt;/code> ressource from lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/output.tf#L26-L28">26-28&lt;/a> and also the &lt;code>kubeone_hosts&lt;/code> from line &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/output.tf#L30-L47">38-47&lt;/a>&lt;/p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
output.tf before
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;p>output &amp;ldquo;ssh_commands&amp;rdquo; {
value = formatlist(&amp;ldquo;ssh ${var.ssh_username}@%s&amp;rdquo;, hcloud_server.control_plane.*.ipv4_address)
}&lt;/p>
&lt;p>output &amp;ldquo;kubeone_hosts&amp;rdquo; {
description = &amp;ldquo;Control plane endpoints to SSH to&amp;rdquo;&lt;/p>
&lt;p>value = {
control_plane = {
hostnames = hcloud_server.control_plane.&lt;em>.name
cluster_name = var.cluster_name
cloud_provider = &amp;ldquo;hetzner&amp;rdquo;
private_address = hcloud_server_network.control_plane.&lt;/em>.ip
public_address = hcloud_server.control_plane.*.ipv4_address
network_id = hcloud_network.net.id
ssh_agent_socket = var.ssh_agent_socket
ssh_port = var.ssh_port
ssh_private_key_file = var.ssh_private_key_file
ssh_user = var.ssh_username
}
}
}&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
output.tf after
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;p>output &amp;ldquo;ssh_commands&amp;rdquo; {
value = [
&amp;ldquo;ssh ${var.ssh_username}@hcloud_server.control_plane_1.ipv4_address&amp;rdquo;,
&amp;ldquo;ssh ${var.ssh_username}@hcloud_server.control_plane_2.ipv4_address&amp;rdquo;,
&amp;ldquo;ssh ${var.ssh_username}@hcloud_server.control_plane_3.ipv4_address&amp;rdquo;
]
}&lt;/p>
&lt;p>output &amp;ldquo;kubeone_hosts&amp;rdquo; {
description = &amp;ldquo;Control plane endpoints to SSH to&amp;rdquo;&lt;/p>
&lt;p>value = {
control_plane = {
cluster_name = var.cluster_name
cloud_provider = &amp;ldquo;hetzner&amp;rdquo;
network_id = hcloud_network.net.id
ssh_agent_socket = var.ssh_agent_socket
ssh_port = var.ssh_port
ssh_private_key_file = var.ssh_private_key_file
ssh_user = var.ssh_username
hostnames = [
hcloud_server.control_plane_1.name,
hcloud_server.control_plane_2.name,
hcloud_server.control_plane_3.name
]
private_address = [
hcloud_server_network.control_plane_1.ip,
hcloud_server_network.control_plane_2.ip,
hcloud_server_network.control_plane_3.ip
]
public_address = [
hcloud_server.control_plane_1.ipv4_address,
hcloud_server.control_plane_2.ipv4_address,
hcloud_server.control_plane_3.ipv4_address
]
}
}
}&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="reliability-tip-3-use-terraform-remote-backends">Reliability Tip 3: Use terraform remote backends&lt;/h2>
&lt;p>I&amp;rsquo;m almost certain, every single one of you ran &lt;code>terraform apply&lt;/code> at least once on their local machine. I mean, after all, that is how terraform is supposed to be used, right?&lt;/p>
&lt;p>And the sad truth is, I&amp;rsquo;ve seen a lot of production environments that where built exactly like that: Someone ran &lt;code>terraform apply&lt;/code> on their local machine. And hey, now we can advertise our infrastructure as &amp;ldquo;infrastructure as code&amp;rdquo;. &lt;em>Technically&lt;/em> this migt be correct but it is certainly not what you would expect.&lt;/p>
&lt;p>To us &amp;ldquo;DevOps&amp;rdquo; or &amp;ldquo;SRE&amp;rdquo; folks it is obvious to run terraform from a CI/CD pipeline.&lt;/p>
&lt;p>GitLab released &lt;a href="https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html">GitLab managed Terraform state&lt;/a> a while back &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, adding a feature allowing you to securely store your &lt;a href="https://www.terraform.io/language/state">tfstate&lt;/a> within GitLab.&lt;/p>
&lt;p>But there is still a problem with this approach: Many don&amp;rsquo;t use GitLab. I use GitHub for the overwhelming majority of my work.
And if the pipeline executes terraform for me, I can&amp;rsquo;t easily run &lt;code>terraform plan&lt;/code> on my local machine to validate changes before pushing them. Or at least not without manually downloading the tfstate first.&lt;/p>
&lt;p>But there is a solution that works from any CI/CD platform as well as the CLI, regardless of platform level integrations that allows for safe storage of the tfstate.&lt;/p>
&lt;p>And that&amp;rsquo;s where the terraform &lt;a href="https://www.terraform.io/language/settings/backends/remote">remote backend&lt;/a> comes in to play.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/language/settings/backends">Backends&lt;/a> in terraform defines where the &lt;a href="https://www.terraform.io/language/state">state&lt;/a> snapshots are stored.&lt;/p>
&lt;p>This particular backend uses &lt;a href="https://app.terraform.io">the terraform cloud&lt;/a> to actually run terraform for you.&lt;/p>
&lt;p>Remote backends give you the greatest level of flexibility and ease as it&amp;rsquo;s possible to use terraform from any (or even multiple) CI/CD pipeline platform(s) and even your local machines without worrying about keeping tfstates, and variables in sync. All Variables (and for that matter secrets as well) are stored on the terraform cloud.&lt;/p>
&lt;p>You can find a tutorial on how to set up the remote backend &lt;a href="https://learn.hashicorp.com/tutorials/terraform/github-actions">here&lt;/a>.&lt;/p>
&lt;h2 id="reliability-tip-4-configure-your-kubeoneyaml-correctly">Reliability Tip 4: Configure your KubeOne.yaml correctly&lt;/h2>
&lt;p>If you&amp;rsquo;re just starting with KubeOne, you might find a KubeOne.yaml that looks like this:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
a basic kubeone.yaml
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: kubeone.io/v1beta1
kind: KubeOneCluster
versions:
kubernetes: &amp;#39;1.19.3&amp;#39;
cloudProvider:
hetzner: {}
external: true
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>But unfortunately - on the KubeOne documentation website - there isn&amp;rsquo;t a great deal of information available on how to configure your KubeOne cluster in more depth.&lt;/p>
&lt;p>But thankfully, the kubeone-cli comes with a nifty command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubeone config print --full
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will show you all available config options with the defaults used.
KubeOne does a good job with those defaults and not much configuration is needed.&lt;/p>
&lt;p>I want to deploy the &amp;ldquo;cluster-autoscaler&amp;rdquo; addon which comes right out of the box with KubeOne and is particularly useful for production-ready clusters.&lt;/p>
&lt;p>I also ensure I set the MTU for canal correctly, as things tend to get a bit icky if the MTU is wrong.&lt;/p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
final kubeone.yaml
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: kubeone.io/v1beta1
kind: KubeOneCluster
versions:
kubernetes: &amp;#39;1.23.1&amp;#39;
clusterNetwork:
cni:
canal:
mtu: 1400 # Hetzner specific 1450 bytes - 50 VXLAN bytes
cloudProvider:
hetzner: {}
external: true
addons:
enable: true
path: &amp;#34;./addons&amp;#34;
addons:
- name: cluster-autoscaler
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;h2 id="reliability-tip-5-configure-canal-to-not-listen-on-the-public-network-interface">Reliability Tip 5: Configure Canal to not listen on the public network interface&lt;/h2>
&lt;p>Well, technically not &lt;code>canal&lt;/code> itself but &lt;code>flanel&lt;/code> which is a part of &lt;code>canal&lt;/code>. Remember: &lt;code>canal&lt;/code> is just a combination of &lt;code>calico&lt;/code> and &lt;code>flanel&lt;/code> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The flanel backend which is shipped as part of your canal CNI installation is by default binding on your internet facing &lt;code>eth0&lt;/code> port&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This is absolutely not what you want!&lt;/p>
&lt;p>I was made aware of the problem by this GitHub issue: &lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849429229">hetznercloud/csi-driver#204&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>Is it possible that your Kubernetes nodes use their public IPs (and interfaces) instead of a private network for communication between the nodes?
&amp;ndash; &lt;em>&lt;a href="https://github.com/ekeih">Max Rosin (@ekeih)&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>and a solution for the problem was pointed out in &lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849567869">this comment&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>In my setup I use -iface-regex=10.0.&lt;em>.&lt;/em> in flannel daemonset
&amp;ndash; &lt;em>&lt;a href="https://github.com/jekakm">Evgeniy Gurinovich (@jekakm)&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>The fix can be applied relatively easy via &lt;code>kubectl patch&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl patch daemonset --namespace kube-system canal --type&lt;span style="color:#f92672">=&lt;/span>json -p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/containers/1/command/-&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;-iface-regex=10\\.0\\.*\\.*&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>daemonset.apps/canal patched
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(It is totally possible to add a step to your CI/CD pipeline that applies the mitigation for you, after you created (or reconciled) your KubeOne cluster)&lt;/p>
&lt;h2 id="reliability-tip-6-deploy-your-worker-nodes-programatically">Reliability Tip 6: Deploy your worker nodes programatically&lt;/h2>
&lt;p>KubeOne comes with a &lt;a href="https://github.com/kubermatic/machine-controller">machine-controller&lt;/a> that can programmatically deploy worker-nodes to hetzner online using the cluster-api&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>KubeOne automagically creates a default &lt;a href="https://cluster-api.sigs.k8s.io/developer/architecture/controllers/machine-deployment.html">machine-deployment&lt;/a> for you.&lt;/p>
&lt;p>It&amp;rsquo;s a good start, but we can do better.
Honestly, I just wouldn&amp;rsquo;t bother modifying the existing machine deployment and just get rid of it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl --namespace kube-system delete machinedeployment prod-ready-pool1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>machinedeployments.cluster.k8s.io/prod-ready-pool1 deleted
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Better create a new machinedeployment.yaml which you should add to your Git-Repo to keep your config in sync.&lt;/p>
&lt;p>When we create our worker-nodes, we want to ensure:&lt;/p>
&lt;ul>
&lt;li>To set annotations for cluster-autoscaler correctly to dynamically scale our worker-nodes&lt;/li>
&lt;li>Deploy our SSH keys to the worker-nodes, allowing easier troubleshooting&lt;/li>
&lt;li>Our worker-nodes are placed in our virtual network&lt;/li>
&lt;li>Labels are added to our worker-nodes, so the hetzner firewall can filter traffic to the worker-nodes as well&lt;/li>
&lt;/ul>
&lt;p>In order for the machinedeployment to work correctly, we therefore need to know a few variables:&lt;/p>
&lt;ul>
&lt;li>The min and max count of worker-nodes&lt;/li>
&lt;li>The cluster-name which is added as a label&lt;/li>
&lt;li>The network-id to place the worker-nodes in the correct virtual network&lt;/li>
&lt;li>The cluster-version as defined in our kubeone.yaml&lt;/li>
&lt;li>The datacenter location (ideally the same as the API servers)&lt;/li>
&lt;/ul>
&lt;p>Luckily terraform already provides us all information and we can obtain the terraform output in JSON format.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ terraform output -json &amp;gt; output.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And now we can determine most of the variables by using a little &lt;code>jq&lt;/code> magic:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export AUTOSCALER_MIN&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export AUTOSCALER_MAX&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export NETWORK_ID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.kubeone_hosts.value.control_plane.network_id&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export CLUSTER_NAME&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.kubeone_hosts.value.control_plane.cluster_name&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export CLUSTER_VERSION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>yq e -j &amp;lt; kubeone.yaml | jq -r &lt;span style="color:#e6db74">&amp;#39;.versions.kubernetes&amp;#39;&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export DATACENTER_LOCATION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.control_plane_info.value.location&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, we can use a template of our machinedeployment and make use of &lt;code>envsubst&lt;/code> to render our template&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
machinedeployment.yaml.tpl
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: &amp;#34;cluster.k8s.io/v1alpha1&amp;#34;
kind: MachineDeployment
metadata:
name: &amp;#34;${CLUSTER_NAME}-node-pool&amp;#34;
namespace: &amp;#34;kube-system&amp;#34;
annotations:
cluster.k8s.io/cluster-api-autoscaler-node-group-min-size: &amp;#34;${AUTOSCALER_MIN}&amp;#34;
cluster.k8s.io/cluster-api-autoscaler-node-group-max-size: &amp;#34;${AUTOSCALER_MAX}&amp;#34;
spec:
paused: false
replicas: ${AUTOSCALER_MIN}
strategy:
type: RollingUpdate
rollingUpdate:
maxSurge: 20%
maxUnavailable: 10%
minReadySeconds: 60
selector:
matchLabels:
node: &amp;#34;${CLUSTER_NAME}&amp;#34;
template:
metadata:
labels:
node: &amp;#34;${CLUSTER_NAME}&amp;#34;
spec:
providerSpec:
value:
cloudProvider: &amp;#34;hetzner&amp;#34;
cloudProviderSpec:
token:
secretKeyRef:
namespace: kube-system
name: cloud-provider-credentials
key: HZ_TOKEN
labels:
role: worker
cluster: &amp;#34;${CLUSTER_NAME}&amp;#34;
serverType: &amp;#34;cpx31&amp;#34;
location: &amp;#34;${DATACENTER_LOCATION}&amp;#34;
image: &amp;#34;ubuntu-20.04&amp;#34;
networks:
- &amp;#34;${NETWORK_ID}&amp;#34;
operatingSystem: &amp;#34;ubuntu&amp;#34;
operatingSystemSpec:
distUpgradeOnBoot: false
sshPublicKeys:
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOO9DMiwRjCCWvMA9TKYxRApgQx3g+owxkq9jy1YyjGN cedi@mae
versions:
kubelet: &amp;#34;${CLUSTER_VERSION}&amp;#34;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>envsubst &amp;lt; ./machinedeployment.yaml.tpl &amp;gt; ./machinedeployment.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="further-reads--additional-links-and-ressources">Further Reads / Additional links and ressources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.terraform.io/language/meta-arguments/count">terraform.io/language/meta-arguments/count&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tomharrisonjr.com/terraform-count-is-a-miserable-hack-d58a6ffbf422">tomharrisonjr.com - Terraform count is a Miserable Hack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849429229">GitHub.com - [hashicorp/terraform#3885] Changing count of instances destroys all of them&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.weave.works/blog/the-definitive-guide-to-kubernetes-in-production">The Definitive Guide to Kubernetes in Production&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521">Understanding Distributed Consensus with Raft&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>[40]:&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a href="https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521">Diego Ongaro and John Ousterhout, In Search of an Understandable Consensus Algorithm (Extended Version), Stanford University&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a href="https://about.gitlab.com/releases/2020/05/22/gitlab-13-0-released/">GitLab 13.0 released with Gitaly Clusters, Epic Hierarchy on Roadmaps, and Auto Deploy to ECS&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a href="https://www.suse.com/c/rancher_blog/comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave/">Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave, Rancher Blog, Suse&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a href="https://github.com/flannel-io/flannel/blob/master/Documentation/configuration.md#key-command-line-options">flanel configuration, flanel documentation, GitHub&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a href="https://cluster-api.sigs.k8s.io">Kubernetes Cluster API&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a href="https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html">Invoking the envsubst program&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>KKPCTL: The Command Line Tool for Kubermatic Kubernetes Platform</title><link>https://cedi.dev/publication/kkpctl/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://cedi.dev/publication/kkpctl/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></description></item></channel></rss>