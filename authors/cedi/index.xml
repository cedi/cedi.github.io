<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cedi.dev</title><link>https://cedi.dev/authors/cedi/</link><description>Recent content on cedi.dev</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year} by Cedric Kienzler</copyright><atom:link href="https://cedi.dev/authors/cedi/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetes - The good, the bad, the ugly</title><link>https://cedi.dev/talk/kubernetes-good-bad-ugly/</link><pubDate>Fri, 20 May 2022 18:45:00 +0000</pubDate><guid>https://cedi.dev/talk/kubernetes-good-bad-ugly/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Understanding Alerting - How to come up with a good enough alerting strategy</title><link>https://cedi.dev/talk/alerting/</link><pubDate>Fri, 20 May 2022 13:00:00 +0000</pubDate><guid>https://cedi.dev/talk/alerting/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>Above-the-line / Below-the-line framework</title><link>https://cedi.dev/post/above-the-line-framework/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/above-the-line-framework/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Welcome back to my blog!&lt;/p>
&lt;p>In this article I want to challenge how you think about your systems design.
The above-the-line / below-the-line framework might appear weird at first, since it is introducing some level of abstraction between you and the system, but I hope while reading trough the article and reflecting on it, you come to terms with the framework.
If you accept it, it will allow you to write better documentation, communicate better about a system architecture, and come up with better monitoring for it.&lt;/p>
&lt;p>I hope you will find this article helpful! If you do - please consider sharing it to your co-workers and friends!&lt;/p>
&lt;h2 id="modelling-a-system">Modelling a system&lt;/h2>
&lt;p>When you are performing any kind of systems-design, you will produce diagrams such as the one below. I would argue, most readers of this are familiar with some sort or another of such diagrams and use them almost daily as part of their job.
A typical systems consists of software components running on various servers and clusters behind layers of load balancers and routers, using databases running in close proximity to the application, connecting to 3rd party APIs over the internet, and presenting results to other systems or users.
&lt;img src="images/system.png" alt="system">&lt;/p>
&lt;p>And there is absolutely nothing wrong with this systems design overview. Except maybe that it is on a very high level.&lt;/p>
&lt;p>But you need some amount of supporting infrastructure in order to run your designed system. Maybe you&amp;rsquo;re depending on a cloud-provider, but most certainly your application code and pipeline definitions exists in repositories. Pipelines are executed by pipeline runners on some other infrastructure. You have additional deployment tools, helper scripts, and so on.&lt;/p>
&lt;p>For simplicity let’s focus on the least amount of supporting infrastructure needed: Version Control Servers, Testing Tools, Pipelines, and issue-trackers.
I think we quickly end up with something like this:
&lt;img src="images/overview.png" alt="overview">&lt;/p>
&lt;p>Yay - now we successfully managed to map out the most important parts of our system. We have the system itself as well as the supporting infrastructure.&lt;/p>
&lt;p>Looks good, right?&lt;/p>
&lt;h2 id="interacting-with-a-system">Interacting with a system&lt;/h2>
&lt;p>Now let’s have a look at the different individuals involved in building, supporting, and using the system. These are the developers, SREs, QA engineers, and program managers.&lt;/p>
&lt;p>They all have different backgrounds and might have different intentions when interacting with the system. While the PMs want to push for new features, QA might push for fewer bugs and the SREs might push for higher reliability and resiliency.&lt;/p>
&lt;p>But how exactly are they going to interact with the system?&lt;/p>
&lt;p>We experience this every day in our jobs. The developers will write code that they push to the repositories in our VCS based on feature-requests of but-tickets in the ticket-system, the SREs will observe the systems state via the monitoring tools to inform decisions about improving reliability, DevOps might improve upon the CI/CD pipelines by editing the YAML files which they push to the VCS for the pipeline-system to pick them up. The QA engineers use the web-frontend of the deployment tool to kick-off a pipeline run deploying to a test-environment, perform their tests using a vast inventory of testing tools and test descriptions. And then there are our PMs who will write JIRA-Tickets to request new features.&lt;/p>
&lt;p>Sounds familiar, doesn&amp;rsquo;t it?
&lt;img src="images/people.png" alt="people">&lt;/p>
&lt;p>And everyone working in this area knows that there is a problem. The great disconnect between QA who thinks the developers can&amp;rsquo;t write bug-free code, the PMs who think the SREs are unreasonable to push back on a release on a Friday evening because the error budget is exhausted for the month … .&lt;/p>
&lt;p>After all, everyone here knows this meme and to some extend relate to it, am I right?:
&lt;img src="images/meme.jpg" alt="how project managers, developers, qa, sysadmin, designers see each other">&lt;/p>
&lt;p>But why is that?&lt;/p>
&lt;p>Because everyone understands the system differently. Everyone has a different mental-model of how the system works. And how it will fail.
No one has a complete understanding of the system.
No one knows exactly how it will fail.
Everyone has different experiences with how the system failed in the past, or how similar systems failed in the past.
This lays in the very nature of every only so slightly complex system. And systems in the area of computer-systems are always complex ones.&lt;/p>
&lt;p>&lt;img src="images/mentalmodels.png" alt="mental models">&lt;/p>
&lt;h2 id="the-_above-the-line--below-the-line_-framework">The &lt;em>above the line / below the line&lt;/em> framework&lt;/h2>
&lt;p>After looking at a system itself and the different people interacting with a system, it is time to combine the two into one model and call with the above the line / below the line framework.&lt;/p>
&lt;p>We call it that, because we draw a line and put the people interacting with the system and their different mental models and intentions above this line and our beloved complex system below this line.&lt;/p>
&lt;p>&lt;img src="images/above-the-line-below-the-line.png" alt="above-the-line/below-the-line">&lt;/p>
&lt;p>We call this (green) line the &lt;em>line of representation&lt;/em>, because whenever we interact with our system, we interact with a representation of our system.&lt;/p>
&lt;p>The consequence of this framework is, that everything &amp;ldquo;below the line&amp;rdquo; is interfered from the mental models of the individuals and therefore that the system itself does not exist in the physical world.
We cannot see, touch, or directly control our system.
The only way we can interact with our system is trough the representation layer.&lt;/p>
&lt;p>I think unconsciously we know this already.&lt;/p>
&lt;p>Why else would we rely on &amp;ldquo;infrastructure as code&amp;rdquo;? And why else would we rather commit our CI/CD Pipeline definition and configurations as YAML files than using a bulky UI (looking at you Jenkins)?
We choose a representation that is familiar to us to define our systems state to make it easier to work with.
It is easier for us to inform our mental model based on a known representation. And the other way round, it is easy to translate our mental-model into a representation that is familiar to us.&lt;/p>
&lt;p>One very important consequence of this model is, that every time someone makes any change to a system, the change is based on the persons mental model of the system at a certain point in time.&lt;/p>
&lt;p>But the data which informs our mental model becomes stale very quickly.
We know that code and configuration changes over time. The same is true for the requirements of our system.
Even the hardware on which system is running changes over time and become less reliable because of bit-rot, degrading optical transceivers, or other variables.
And finally 3rd party application behavior changes over time thanks to updates, causing a change in the access pattern of your drives or how it uses the network.&lt;/p>
&lt;p>We have to accept and live with a constantly changing system.&lt;/p>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>We can&amp;rsquo;t talk about this framework without talking about challenges that arise as a consequence of this system.&lt;/p>
&lt;p>I think the most important two are&lt;/p>
&lt;ol>
&lt;li>Every individual working with a system must develop and maintain a good-enough mental model of the system.&lt;/li>
&lt;li>Every individual working with a system must develop and maintain a good-enough understanding of other individuals mental-models of the system.&lt;/li>
&lt;/ol>
&lt;p>For new-hires we have the Onboarding-phase in which a new-hire is expected to learn everything about the system. During this phase we build our initial mental-model of the system.&lt;/p>
&lt;p>For everyone else we have knowledge-sharing-sessions or something like that.
We have architecture-deep-dive sessions with our peers, or conduct systems-design meetings when planning for big-changes.
And finally we review each-others merge requests.&lt;/p>
&lt;p>We don&amp;rsquo;t do all this for the sake of spotting someone else’s mistake.&lt;/p>
&lt;p>Instead we validate each others assumptions about how the system behaves and therefore refine each-others mental-models about the system.&lt;/p>
&lt;p>What might appears to us as a &amp;ldquo;design-flaw&amp;rdquo; or &amp;ldquo;error&amp;rdquo; in someone else’s design or code-change might not be because they &amp;ldquo;did a mistake&amp;rdquo; but because their mental-model of how the system works and or fails was flawed.&lt;/p>
&lt;p>As mentioned above:&lt;/p>
&lt;blockquote>
&lt;p>Every time a system surprises us, that is because our mental model is flawed and or information became stale.&lt;/p>
&lt;/blockquote>
&lt;p>This is also true for reviews. If we are surprised by a suggestion to change the design or code, that is likely because our own mental-model isn’t aligned with the mental-model of the person which suggestion you are reviewing.&lt;/p>
&lt;p>In my opinion, frequent architecture or design reviews, as well as active participation in merge request reviews are crucial, because this helps us to inform each-other about our mental-models and it makes it easier to get a feeling and understanding of how others form their mental-model.&lt;/p></description></item><item><title>What the heck are devcontainers?</title><link>https://cedi.dev/post/devcontainer-pt1/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/devcontainer-pt1/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hi and welcome back to my blog.&lt;/p>
&lt;p>I think it&amp;rsquo;s about time to talk about an amazing feature of VScode that I rarely see used in the wild. And honestly: I do not understand why few are using it.
You might have guessed it from the title. The feature is called &amp;ldquo;devcontainers&amp;rdquo;.&lt;/p>
&lt;p>But you might ask: &amp;ldquo;What are devcontainers?&amp;rdquo; and &amp;ldquo;Why should I care?&amp;rdquo;.&lt;/p>
&lt;p>I&amp;rsquo;m about to explain devcontainers in a small series of blog-posts, starting with this one.&lt;/p>
&lt;p>In this post I am going to talk about what devcontainers are, why you should care, and what the advantage of devcontainers are.
After reading this I hope you want to add a devcontainer to every Git Repository you work with.&lt;/p>
&lt;p>In a second part, I will continue our journey with devcontainers and talk about how to set-up a devcontainer and what resources are available to you when creating them.&lt;/p>
&lt;h2 id="what-are-devcontainers">What are devcontainers?&lt;/h2>
&lt;p>Technically, devcontainers are ordinary (Docker) containers.
But it&amp;rsquo;s not &lt;em>any&lt;/em> Docker container. It&amp;rsquo;s a Docker container that is you virtualised development environment. You install all of your development tools (like compilers, code generators, doc generators, and so on) in the devcontainer instead of installing all that on your local machine.&lt;/p>
&lt;p>And using devcontainers (using the &lt;code>devcontainer.json&lt;/code> config) you can even configure VScode. But not only configuration, the configuration can (and should) specify the extensions that will be automatically installed.&lt;/p>
&lt;p>And the nice thing: If you have the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">&lt;code>ms-vscode-remote.remote-containers&lt;/code>&lt;/a> extension installed and you open a folder in VScode that has a &lt;code>.devcontainer&lt;/code> folder inside of it, and that folder contains a &lt;code>devcontainer.json&lt;/code> and a &lt;code>Dockerfile&lt;/code> VScode will ask you if you want to reopen VScode in the devcontainer:&lt;/p>
&lt;p>&lt;img src="images/devcontainer-reopen.png" alt="devcontainer-reopen">&lt;/p>
&lt;p>If it doesn&amp;rsquo;t prompt you, you can select it from the command palette:&lt;/p>
&lt;p>&lt;img src="images/reopen-command.png" alt="devcontainer-reopen">&lt;/p>
&lt;p>You can give it a try right now. The &lt;a href="https://github.com/cedi/cedi.github.io/">GitHub Repository&lt;/a> hosting this blog comes with a &lt;a href="https://github.com/cedi/cedi.github.io/tree/main/.devcontainer">devcontainer&lt;/a>.&lt;/p>
&lt;h2 id="why-should-you-care">Why should you care?&lt;/h2>
&lt;p>If you&amp;rsquo;re anything like me, it&amp;rsquo;s likely you work with a multitude of programming languages, frameworks, and tools.
You have dozens of repositories checked out and about enough VScode plugins installed, the VScode startup time is comparable to the startup time of VisualStudio 2022 Ultimate Edition.&lt;/p>
&lt;p>But it doesn&amp;rsquo;t have to be like this.
With Devcontainers you have the bare minimum VScode installation with the only little configuration or plugins. Startup times are blazingly fast.&lt;/p>
&lt;p>You don&amp;rsquo;t have to worry about that ruby installation that you forgotten about because you tried to get Jekyll running before finally giving up and switching to Hugo.
You don&amp;rsquo;t have to care if you have one codebase uses .NET 4 while the other codebase uses .NET Core 5.
Because that is all taken care of inside of you devcontainer.&lt;/p>
&lt;p>Imagine you&amp;rsquo;re a new-hire in your company or a vendor joining a new project, you have to install a dozen different compilers, frameworks, libraries, tools to test, and tools to refactor your code. How long does it take you to get everything setup and being ready to get started? A day, or two?
Imagine you&amp;rsquo;re the Manager of a team and every new-hire or new contractor requires multiple days to get his workstation set-up.&lt;/p>
&lt;p>The time wasted with setting up your development environment is almost unbearable. But the solution is simple enough: Put the entire development environment inside of a devcontainer.&lt;/p>
&lt;p>The entire setup process would be installing&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://git-scm.com/downloads">Git&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/get-docker/">Docker Desktop&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://code.visualstudio.com/download">VScode&lt;/a>&lt;/li>
&lt;li>The &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">&lt;code>ms-vscode-remote.remote-containers&lt;/code>&lt;/a> extension&lt;/li>
&lt;li>Clone the repository&lt;/li>
&lt;li>open it in VScode&lt;/li>
&lt;/ol>
&lt;p>and boom — you are done!&lt;/p>
&lt;p>Wouldn&amp;rsquo;t that be amazing?&lt;/p>
&lt;p>Stay tuned for part 2, when we talk about how to set-up a devcontainer.&lt;/p>
&lt;h2 id="codespaces">Codespaces&lt;/h2>
&lt;p>The last reason for using Devcontainers is &lt;a href="https://github.com/features/codespaces">Codespaces&lt;/a>.
If you never heard of Codespaces before, here is a quick summary from the &lt;a href="https://github.com/features/codespaces">GitHub Codespaces page&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Use the full power of Visual Studio Code, including the editor, terminal, debugger, version control, settings sync, and the entire ecosystem of extensions. Work in the browser or hand off to your desktop.&lt;/p>
&lt;p>Spin up new dev environment for any sized project in seconds with prebuilt images. GitHub’s own 35GB dev image starts in under 10 seconds. Scale your cloud VMs up to 32 cores and 64GB of RAM. And with low-latency connections across four regions, you won’t even remember it’s not your local machine.&lt;/p>
&lt;/blockquote>
&lt;p>If your repository contains a Devcontainer and you’re hosting your Repository on GitHub, you can make use of Codespaces without any further setup required.
No need to install anything on your local machine anymore — heck, you could even code from your iPad.&lt;/p></description></item><item><title>Bootstrapping a production ready Kubernetes on Hetzner Cloud</title><link>https://cedi.dev/post/prod-ready-kubeone/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://cedi.dev/post/prod-ready-kubeone/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hi and welcome to my first blog post on my new website.&lt;/p>
&lt;p>I know what you are thinking right now &amp;ldquo;Oh no! Not another blogpost about setting up a Kubernetes Cluster!&amp;rdquo;.
And yeah, I get it! There are a lot of blog-posts, tutorials, and articles already written about this topic.
For example &lt;a href="https://shibumi.dev">shibumi&lt;/a> wrote an amazing blog-post about &lt;a href="https://shibumi.dev/posts/kubernetes-on-hetzner-in-2021">Kubernetes on Hetzner in 2021&lt;/a>, and there is a even a &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/hetzner">Hetzner example terraform&lt;/a> in the &lt;a href="https://github.com/kubermatic/kubeone">KubeOne GitHub&lt;/a>.&lt;/p>
&lt;p>But here is the deal: while those posts and examples give you an &lt;em>easy&lt;/em> quick-start, they don&amp;rsquo;t cover the aspects of bootstrapping a KubeOne cluster that is supposed to run in production some day.&lt;/p>
&lt;p>To scope this blog-post a bit down, I have to make a few assumptions:&lt;/p>
&lt;ul>
&lt;li>You already have KubeOne installed on your local machine&lt;/li>
&lt;li>You have a project on Hetzner Cloud&lt;/li>
&lt;li>You have already created a Hetzner API Token for your Project with read+write permissions, or you are able to do so&lt;/li>
&lt;li>You want to use &lt;strong>Canal&lt;/strong> as your CNI of choice and not Cilium which was recently added in &lt;a href="https://github.com/kubermatic/kubeone/releases/tag/v1.4.0">KubeOne 1.4&lt;/a>.&lt;/li>
&lt;li>You are familiar with &lt;a href="https://www.terraform.io">terraform&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="preface-and-acknowledgements">Preface and Acknowledgements&lt;/h2>
&lt;p>This is not a &amp;ldquo;definitive guide&amp;rdquo; nor should you take everything you read too serious. I might be wrong or too opinionated about stuff.&lt;/p>
&lt;p>Actually running Kubernetes in production is way harder than just reading this article. I intentionally leave out &lt;strong>many&lt;/strong> details of how to actually run Kubernetes in production.&lt;/p>
&lt;p>Specifically, in this post I will not talk about:&lt;/p>
&lt;ul>
&lt;li>Security&lt;/li>
&lt;li>GitOps&lt;/li>
&lt;li>Disaster recovery&lt;/li>
&lt;li>Monitoring here&lt;/li>
&lt;/ul>
&lt;p>I might dedicate future blog-posts to those topics (and hopefully remember to link them back here).&lt;/p>
&lt;p>Therefore, the scope of this blog post is narrowed down to bootstrapping a Kubernetes Cluster using KubeOne - with somewhat sane defaults and measures taken - that will get you started on your journey to a production ready Kubernetes Cluster.&lt;/p>
&lt;h2 id="reliability-tip-1-use-an-odd-number-of-api-servers">Reliability Tip 1: Use an odd number of API servers&lt;/h2>
&lt;p>To get started we need to have some virtual servers running on Hetzner Cloud to install the Kubernetes API Server, etcd database and the cluster&amp;rsquo;s control-plane on.
You should stick to odd numbers of your API servers because etcd needs a majority of nodes to agree on updates to the cluster state.&lt;/p>
&lt;p>This majority (quorum) required for etcd is &lt;code>(n/2)+1&lt;/code> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The &lt;a href="https://etcd.io/docs/v3.3/faq/#why-an-odd-number-of-cluster-members">etcd FAQ&lt;/a> page describes it:&lt;/p>
&lt;blockquote>
&lt;p>For any odd-sized cluster, adding one node will always increase the number of nodes necessary for quorum. Although adding a node to an odd-sized cluster appears better since there are more machines, the fault tolerance is worse since exactly the same number of nodes may fail without losing quorum but there are more nodes that can fail. If the cluster is in a state where it can’t tolerate any more failures, adding a node before removing nodes is dangerous because if the new node fails to register with the cluster (e.g., the address is misconfigured), quorum will be permanently lost.&lt;/p>
&lt;/blockquote>
&lt;h2 id="reliability-tip-2-dont-use-the-count-meta-argument-of-terraform">Reliability Tip 2: Don&amp;rsquo;t use the &lt;code>count&lt;/code> meta-argument of terraform&lt;/h2>
&lt;p>Fortunate for us, KubeOne comes with a great set of &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/">examples&lt;/a> for using terraform to set up your infrastructure. We will use the &lt;a href="https://github.com/kubermatic/kubeone/tree/master/examples/terraform/hetzner">hetzner example&lt;/a> as a base and customize it a bit.
It comes with mostly sane defaults and best practices out of the box, including a firewall and a &lt;a href="https://docs.hetzner.com/cloud/placement-groups/overview">placement group&lt;/a> for our control-plane nodes to ensure higher reliability.&lt;/p>
&lt;p>But there is a problem with this example: The usage of the &lt;a href="https://www.terraform.io/language/meta-arguments/count">&lt;code>count&lt;/code> meta-argument&lt;/a> for the &lt;a href="https://github.com/kubermatic/kubeone/blob/bbdceaff5ab1d3360f7455ab5f81dbfde73bf161/examples/terraform/hetzner/main.tf#L111">hcloud_server&lt;/a> definition.&lt;/p>
&lt;p>At first this seems absolutely valid and an easy fix for avoiding duplicate code. But the devil lies within the details as you&amp;rsquo;re about to find out.&lt;/p>
&lt;p>Let&amp;rsquo;s say we want to change the location of our servers, update the base images of our servers, or add/remove an SSH key.
All those changes got something in common:
They will absolutely destroy and re-create the server.&lt;/p>
&lt;p>But because we are using the &lt;code>count&lt;/code> meta-argument, we cannot update (re-create) only one server at a time. We can only replace all servers simultanously.&lt;/p>
&lt;p>KubeOne deploys the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">etcd&lt;/a> database on the API servers, which means if we loose all three API server nodes at the same time, we will loose all three etcd database replicas as well. And if we loose etcd, we loose our entire cluster.&lt;/p>
&lt;p>Therefore, we need to replace everything with a &lt;code>count&lt;/code> meta-argument with an explicit object.
Yes, this causes code duplication.
But it allows us to upgrade one server at a time. And after every server update, we can re-run &lt;code>kubeone&lt;/code> to repair (or &lt;em>reconcile&lt;/em>) our cluster. By doing so, we perform a &lt;em>rolling update&lt;/em> of our control plane without loosing any data.&lt;/p>
&lt;p>Now, lets get to it. We have to change the code blocks in lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L95-L99">95-99&lt;/a>, lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L110-L126">110-126&lt;/a>, and lines &lt;a href="https://github.com/kubermatic/kubeone/blob/56f84d7c6c98760042a37aea2614fac3c783812c/examples/terraform/hetzner/main.tf#L144-L154">144-154&lt;/a>.&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
main.tf before
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane&amp;#34; {
count = var.control_plane_replicas
server_id = element(hcloud_server.control_plane.*.id, count.index)
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane&amp;#34; {
count = var.control_plane_replicas
name = &amp;#34;${var.cluster_name}-control-plane-${count.index + 1}&amp;#34;
server_type = var.control_plane_type
image = var.image
location = var.datacenter
placement_group_id = hcloud_placement_group.control_plane.id
ssh_keys = [
hcloud_ssh_key.kubeone.id,
]
labels = {
&amp;#34;kubeone_cluster_name&amp;#34; = var.cluster_name
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target&amp;#34; {
count = var.control_plane_replicas
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = element(hcloud_server.control_plane.*.id, count.index)
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane,
hcloud_load_balancer_network.load_balancer
]
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>We have to remove the &lt;code>count&lt;/code> meta-argument, get rid of the &lt;code>element(..., count.index)&lt;/code> syntax and replace everything with actual references to explicit objects, so it looks like this:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
main.tf after
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_1&amp;#34; {
name = &amp;#34;api-1&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_1&amp;#34; {
server_id = hcloud_server.control_plane_1.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp1&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_1.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_1,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_1
]
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_2&amp;#34; {
name = &amp;#34;api-2&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_2&amp;#34; {
server_id = hcloud_server.control_plane_2.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp2&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_2.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_2,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_2
]
}
resource &amp;#34;hcloud_server&amp;#34; &amp;#34;control_plane_3&amp;#34; {
name = &amp;#34;api-3&amp;#34;
server_type = &amp;#34;cx21&amp;#34;
image = &amp;#34;ubuntu-20.04&amp;#34;
location = &amp;#34;nbg1&amp;#34;
placement_group_id = hcloud_placement_group.control_plane_placement.id
ssh_keys = [
hcloud_ssh_key.cedi_mae.name
]
labels = {
&amp;#34;cluster&amp;#34; = &amp;#34;k1&amp;#34;
&amp;#34;role&amp;#34; = &amp;#34;api&amp;#34;
}
}
resource &amp;#34;hcloud_server_network&amp;#34; &amp;#34;control_plane_3&amp;#34; {
server_id = hcloud_server.control_plane_3.id
subnet_id = hcloud_network_subnet.kubeone.id
}
resource &amp;#34;hcloud_load_balancer_target&amp;#34; &amp;#34;load_balancer_target_cp3&amp;#34; {
type = &amp;#34;server&amp;#34;
load_balancer_id = hcloud_load_balancer.load_balancer.id
server_id = hcloud_server.control_plane_3.id
use_private_ip = true
depends_on = [
hcloud_server_network.control_plane_3,
hcloud_load_balancer_network.load_balancer,
hcloud_server.control_plane_3
]
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;h2 id="reliability-tip-3-use-terraform-remote-backends">Reliability Tip 3: Use terraform remote backends&lt;/h2>
&lt;p>I&amp;rsquo;m almost certain, every single one of you ran &lt;code>terraform apply&lt;/code> at least once on their local machine. I mean, after all, that is how terraform is supposed to be used, right?&lt;/p>
&lt;p>And the sad truth is, I&amp;rsquo;ve seen a lot of production environments that where built exactly like that: Someone ran &lt;code>terraform apply&lt;/code> on their local machine. And hey, now we can advertise our infrastructure as &amp;ldquo;infrastructure as code&amp;rdquo;. &lt;em>Technically&lt;/em> this migt be correct but it is certainly not what you would expect.&lt;/p>
&lt;p>To us &amp;ldquo;DevOps&amp;rdquo; or &amp;ldquo;SRE&amp;rdquo; folks it is obvious to run terraform from a CI/CD pipeline.&lt;/p>
&lt;p>GitLab released &lt;a href="https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html">GitLab managed Terraform state&lt;/a> a while back &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, adding a feature allowing you to securely store your &lt;a href="https://www.terraform.io/language/state">tfstate&lt;/a> within GitLab.&lt;/p>
&lt;p>But there is still a problem with this approach: Many don&amp;rsquo;t use GitLab. I use GitHub for the overwhelming majority of my work.
And if the pipeline executes terraform for me, I can&amp;rsquo;t easily run &lt;code>terraform plan&lt;/code> on my local machine to validate changes before pushing them. Or at least not without manually downloading the tfstate first.&lt;/p>
&lt;p>But there is a solution that works from any CI/CD platform as well as the CLI, regardless of platform level integrations that allows for safe storage of the tfstate.&lt;/p>
&lt;p>And that&amp;rsquo;s where the terraform &lt;a href="https://www.terraform.io/language/settings/backends/remote">remote backend&lt;/a> comes in to play.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/language/settings/backends">Backends&lt;/a> in terraform defines where the &lt;a href="https://www.terraform.io/language/state">state&lt;/a> snapshots are stored.&lt;/p>
&lt;p>This particular backend uses &lt;a href="https://app.terraform.io">the terraform cloud&lt;/a> to actually run terraform for you.&lt;/p>
&lt;p>Remote backends give you the greatest level of flexibility and ease as it&amp;rsquo;s possible to use terraform from any (or even multiple) CI/CD pipeline platform(s) and even your local machines without worrying about keeping tfstates, and variables in sync. All Variables (and for that matter secrets as well) are stored on the terraform cloud.&lt;/p>
&lt;p>You can find a tutorial on how to set up the remote backend &lt;a href="https://learn.hashicorp.com/tutorials/terraform/github-actions">here&lt;/a>.&lt;/p>
&lt;h2 id="reliability-tip-4-configure-your-kubeoneyaml-correctly">Reliability Tip 4: Configure your KubeOne.yaml correctly&lt;/h2>
&lt;p>If you&amp;rsquo;re just starting with KubeOne, you might find a KubeOne.yaml that looks like this:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
a basic kubeone.yaml
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: kubeone.io/v1beta1
kind: KubeOneCluster
versions:
kubernetes: &amp;#39;1.19.3&amp;#39;
cloudProvider:
hetzner: {}
external: true
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>But unfortunately - on the KubeOne documentation website - there isn&amp;rsquo;t a great deal of information available on how to configure your KubeOne cluster in more depth.&lt;/p>
&lt;p>But thankfully, the kubeone-cli comes with a nifty command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubeone config print --full
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will show you all available config options with the defaults used.
KubeOne does a good job with those defaults and not much configuration is needed.&lt;/p>
&lt;p>I want to deploy the &amp;ldquo;cluster-autoscaler&amp;rdquo; addon which comes right out of the box with KubeOne and is particularly useful for production-ready clusters.&lt;/p>
&lt;p>I also ensure I set the MTU for canal correctly, as things tend to get a bit icky if the MTU is wrong.&lt;/p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
final kubeone.yaml
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: kubeone.io/v1beta1
kind: KubeOneCluster
versions:
kubernetes: &amp;#39;1.23.1&amp;#39;
clusterNetwork:
cni:
canal:
mtu: 1400 # Hetzner specific 1450 bytes - 50 VXLAN bytes
cloudProvider:
hetzner: {}
external: true
addons:
enable: true
path: &amp;#34;./addons&amp;#34;
addons:
- name: cluster-autoscaler
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;h2 id="reliability-tip-5-configure-canal-to-not-listen-on-the-public-network-interface">Reliability Tip 5: Configure Canal to not listen on the public network interface&lt;/h2>
&lt;p>Well, technically not &lt;code>canal&lt;/code> itself but &lt;code>flanel&lt;/code> which is a part of &lt;code>canal&lt;/code>. Remember: &lt;code>canal&lt;/code> is just a combination of &lt;code>calico&lt;/code> and &lt;code>flanel&lt;/code> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The flanel backend which is shipped as part of your canal CNI installation is by default binding on your internet facing &lt;code>eth0&lt;/code> port&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This is absolutely not what you want!&lt;/p>
&lt;p>I was made aware of the problem by this GitHub issue: &lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849429229">hetznercloud/csi-driver#204&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>Is it possible that your Kubernetes nodes use their public IPs (and interfaces) instead of a private network for communication between the nodes?
&amp;ndash; &lt;em>&lt;a href="https://github.com/ekeih">Max Rosin (@ekeih)&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>and a solution for the problem was pointed out in &lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849567869">this comment&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>In my setup I use -iface-regex=10.0.&lt;em>.&lt;/em> in flannel daemonset
&amp;ndash; &lt;em>&lt;a href="https://github.com/jekakm">Evgeniy Gurinovich (@jekakm)&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>The fix can be applied relatively easy via &lt;code>kubectl patch&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl patch daemonset --namespace kube-system canal --type&lt;span style="color:#f92672">=&lt;/span>json -p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/containers/1/command/-&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;-iface-regex=10\\.0\\.*\\.*&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>daemonset.apps/canal patched
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(It is totally possible to add a step to your CI/CD pipeline that applies the mitigation for you, after you created (or reconciled) your KubeOne cluster)&lt;/p>
&lt;h2 id="reliability-tip-6-deploy-your-worker-nodes-programatically">Reliability Tip 6: Deploy your worker nodes programatically&lt;/h2>
&lt;p>KubeOne comes with a &lt;a href="https://github.com/kubermatic/machine-controller">machine-controller&lt;/a> that can programmatically deploy worker-nodes to hetzner online using the cluster-api&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>KubeOne automagically creates a default &lt;a href="https://cluster-api.sigs.k8s.io/developer/architecture/controllers/machine-deployment.html">machine-deployment&lt;/a> for you.&lt;/p>
&lt;p>It&amp;rsquo;s a good start, but we can do better.
Honestly, I just wouldn&amp;rsquo;t bother modifying the existing machine deployment and just get rid of it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl --namespace kube-system delete machinedeployment prod-ready-pool1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>machinedeployments.cluster.k8s.io/prod-ready-pool1 deleted
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Better create a new machinedeployment.yaml which you should add to your Git-Repo to keep your config in sync.&lt;/p>
&lt;p>When we create our worker-nodes, we want to ensure:&lt;/p>
&lt;ul>
&lt;li>To set annotations for cluster-autoscaler correctly to dynamically scale our worker-nodes&lt;/li>
&lt;li>Deploy our SSH keys to the worker-nodes, allowing easier troubleshooting&lt;/li>
&lt;li>Our worker-nodes are placed in our virtual network&lt;/li>
&lt;li>Labels are added to our worker-nodes, so the hetzner firewall can filter traffic to the worker-nodes as well&lt;/li>
&lt;/ul>
&lt;p>In order for the machinedeployment to work correctly, we therefore need to know a few variables:&lt;/p>
&lt;ul>
&lt;li>The min and max count of worker-nodes&lt;/li>
&lt;li>The cluster-name which is added as a label&lt;/li>
&lt;li>The network-id to place the worker-nodes in the correct virtual network&lt;/li>
&lt;li>The cluster-version as defined in our kubeone.yaml&lt;/li>
&lt;li>The datacenter location (ideally the same as the API servers)&lt;/li>
&lt;/ul>
&lt;p>Luckily terraform already provides us all information and we can obtain the terraform output in JSON format.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ terraform output -json &amp;gt; output.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And now we can determine most of the variables by using a little &lt;code>jq&lt;/code> magic:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export AUTOSCALER_MIN&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export AUTOSCALER_MAX&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export NETWORK_ID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.kubeone_hosts.value.control_plane.network_id&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export CLUSTER_NAME&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.kubeone_hosts.value.control_plane.cluster_name&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export CLUSTER_VERSION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>yq e -j &amp;lt; kubeone.yaml | jq -r &lt;span style="color:#e6db74">&amp;#39;.versions.kubernetes&amp;#39;&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export DATACENTER_LOCATION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">`&lt;/span>jq -r &lt;span style="color:#e6db74">&amp;#39;.control_plane_info.value.location&amp;#39;&lt;/span> output.json&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, we can use a template of our machinedeployment and make use of &lt;code>envsubst&lt;/code> to render our template&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>:&lt;/p>
&lt;p>
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
machinedeployment.yaml.tpl
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;pre tabindex="0">&lt;code>apiVersion: &amp;#34;cluster.k8s.io/v1alpha1&amp;#34;
kind: MachineDeployment
metadata:
name: &amp;#34;${CLUSTER_NAME}-node-pool&amp;#34;
namespace: &amp;#34;kube-system&amp;#34;
annotations:
cluster.k8s.io/cluster-api-autoscaler-node-group-min-size: &amp;#34;${AUTOSCALER_MIN}&amp;#34;
cluster.k8s.io/cluster-api-autoscaler-node-group-max-size: &amp;#34;${AUTOSCALER_MAX}&amp;#34;
spec:
paused: false
replicas: ${AUTOSCALER_MIN}
strategy:
type: RollingUpdate
rollingUpdate:
maxSurge: 20%
maxUnavailable: 10%
minReadySeconds: 60
selector:
matchLabels:
node: &amp;#34;${CLUSTER_NAME}&amp;#34;
template:
metadata:
labels:
node: &amp;#34;${CLUSTER_NAME}&amp;#34;
spec:
providerSpec:
value:
cloudProvider: &amp;#34;hetzner&amp;#34;
cloudProviderSpec:
token:
secretKeyRef:
namespace: kube-system
name: cloud-provider-credentials
key: HZ_TOKEN
labels:
role: worker
cluster: &amp;#34;${CLUSTER_NAME}&amp;#34;
serverType: &amp;#34;cpx31&amp;#34;
location: &amp;#34;${DATACENTER_LOCATION}&amp;#34;
image: &amp;#34;ubuntu-20.04&amp;#34;
networks:
- &amp;#34;${NETWORK_ID}&amp;#34;
operatingSystem: &amp;#34;ubuntu&amp;#34;
operatingSystemSpec:
distUpgradeOnBoot: false
sshPublicKeys:
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOO9DMiwRjCCWvMA9TKYxRApgQx3g+owxkq9jy1YyjGN cedi@mae
versions:
kubelet: &amp;#34;${CLUSTER_VERSION}&amp;#34;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>envsubst &amp;lt; ./machinedeployment.yaml.tpl &amp;gt; ./machinedeployment.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="further-reads--additional-links-and-ressources">Further Reads / Additional links and ressources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.terraform.io/language/meta-arguments/count">terraform.io/language/meta-arguments/count&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tomharrisonjr.com/terraform-count-is-a-miserable-hack-d58a6ffbf422">tomharrisonjr.com - Terraform count is a Miserable Hack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-849429229">GitHub.com - [hashicorp/terraform#3885] Changing count of instances destroys all of them&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.weave.works/blog/the-definitive-guide-to-kubernetes-in-production">The Definitive Guide to Kubernetes in Production&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521">Understanding Distributed Consensus with Raft&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a href="https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521">Diego Ongaro and John Ousterhout, In Search of an Understandable Consensus Algorithm (Extended Version), Stanford University&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a href="https://about.gitlab.com/releases/2020/05/22/gitlab-13-0-released/">GitLab 13.0 released with Gitaly Clusters, Epic Hierarchy on Roadmaps, and Auto Deploy to ECS&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a href="https://www.suse.com/c/rancher_blog/comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave/">Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave, Rancher Blog, Suse&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a href="https://github.com/flannel-io/flannel/blob/master/Documentation/configuration.md#key-command-line-options">flanel configuration, flanel documentation, GitHub&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a href="https://cluster-api.sigs.k8s.io">Kubernetes Cluster API&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a href="https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html">Invoking the envsubst program&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>KKPCTL: The Command Line Tool for Kubermatic Kubernetes Platform</title><link>https://cedi.dev/publication/kkpctl/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://cedi.dev/publication/kkpctl/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></description></item></channel></rss>